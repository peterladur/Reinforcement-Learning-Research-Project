\documentclass{report}
\usepackage{amsmath, amssymb, amsthm} % Essential math tools
\usepackage[table]{xcolor}
\usepackage{cleveref}   
\usepackage[utf8]{inputenc}
\usepackage{makecell} % Required for multi-line cells\
\usepackage{comment}
\usepackage{listings}
\usepackage{graphicx}

\title{\textbf{Q-Learning}}
\author{Peter (Petr) Ladur\\\\56445760}
\date{}


\begin{document}
\maketitle
 
\chapter{Introduction}
The aim of this project is to explore Q-Learning in the context of tic-tac-toe and other applications using python. Theoretical Q-Table was generated using the ``minimax'' algorithm, and was compared to Q-Tables generated through training against both, random and perfect opponent. Results of agents trained on different policies playing against each other were recorded. How hyperparameters ($\alpha$, $\tau$) affect training results was investigated and an optimal function for hyperparameters was proposed. Deep-Q-Learning was investigated in the context of different games in the ``Open-AI gymnasium'' (\textit{I know that this is unlikely but if I finish the report and still have some time left over, I will try playing around with another application of Deep Q-Learning such as the stock market}). 

\chapter{Background}
\section{Q-Table}
Q-Table is a table which contains contains expected outcomes after every possible action in a particular state. In the context of tic-tac-toe the state is the game position at that particular moment, and actions are all the empty squares. As seen in \cref{tab:perfect_Q_Table_entry}, for that particualr state there are two actions that lead to a win and three actions which lead to a draw (assuming perfect play). 

%Q-Table example
\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
% Draws top line starting from 2nd column
\cline{2-10}
\multicolumn{1}{c|}{} & (1, 1) & (1, 2) & (1, 3) & (2, 1) & (2, 2) & (2, 3) & (3, 1) & (3, 2) & (3, 3) \\ \hline
\makecell{\_ \_ o \\ \_ x \_ \\ x o \_}  & 
% This is the "No-Package" multi-line trick:
\cellcolor{black!60}& 
1 & 1 & \cellcolor{black!60} & \cellcolor{black!60} & 0 & 0 & 0 & \cellcolor{black!60}\\ \hline
\end{tabular}
% 3. ADD THE CAPTION AND LABEL HERE
\caption{Two actions lead to a win, three actions lead to a draw assuming perfect play from both sides}
\label{tab:perfect_Q_Table_entry} 
\end{table}
 
\section{Q-Learning}
In the context of Q-Learning, the Q-Table can be filled up by playing games and updating the values of actions taken during that game according to the Bellman equation. 

\begin{align}
Q(s_{t}, a) \leftarrow (1-\alpha) \cdot Q(s_{t}, a) + \alpha (r + \gamma \cdot \max_{a}{Q(s_{t + 1}, a)}) 
\end{align}

Where:
\begin{itemize}
    \item $Q(s, a)$ is the predicted outcome for the action at a particular state $s$
    \item $\alpha$ is the learning rate
    \item $\gamma$ is the discount factor
    \item $r$ is the immidiate reward for achiveing a state
\end{itemize}

The learning rate $\alpha$ determines how much does the new information replace the already known information. A learning rate of $1$ would mean that each training iteration the $Q(s, a)$ is completely replaced by $(r + \gamma \cdot \max_{a}{Q(s_{t + 1}, a)})$. A learning rate of $0$ would imply that $Q(s, a)$ is not affected by training at all. The learning rate can be adjusted during training as the model becomes more stable.\\

The immidiate reward in the context of tic-tac-toe is $1$ for a game that is won by \text{X}, $0$ for a game that is drawn and $-1$ for a game that is won by \text{O}.

\begin{comment}
\section{Tic-Tac-Toe}
Tic-Tac-Toe is a simple two-player game on a 3 by 3 grid. The game originated in ancient Egypt atleast 1300 BC. Players take turns placing X's and O's in the grid with the goal of getting 3 in a row. The game is drawn with perfect play from both sides, but O's have to be precise to garantee a draw. Using the ``minimax'' algorithm -assuming the oppponent will play the optimal move- a theoretical Q-Table can be derived.
\end{comment}



\chapter{Q-Learning on tic-tac-toe}

\section{Generating theoretical Q-Table}
The ``minimax'' algorithm can be used in turn-based games to calculate the best policy. It is designed to minimise the potential loss and maximise the potential gain. The algorithm works by first generating all the possible final states and then taking turns ``undoing'' the moves that could have lead to that state by removing \textbf{X}'s and \textbf{O}'s until the initial state (empty board) is reached. Each time an \textbf{X} or \textbf{O} is removed the worst case scenario is considered, filling up the theoretical Q-Table.

\begin{lstlisting}
terminal_states = generate_all_terminal_states()

for parent_state in parent_states:
    
    for x/o in parent_state:

        child_state = parent_state with an x/o removed

        if child_state is terminal_state:
            Q_Table[child_state] = [game_result] * 9
        else:
            \\min or max depends on whether it's x/o removed respectively 
            Q_Table[child_state][index of x/o] = min/max(Q_Table[parent_state])  

            parent_states = child states
\end{lstlisting}

\section{Q-Learning implementation}

During the Q-Learning algorihtm batches of games were played. The agent would choose moves using a weighted probability values according to a slightly modified bolztman function as seen in \eqref{modified bolztman}. States $s$, actions taken $a$ and results $r$ are stored in a queue. The queue is then emptied and Q-Table is updated using the Bellman equation as seen in \eqref{terminal state update} and \eqref{non terminal state update}.\\\\

\begin{align}
    p_{i}(\tau) = \frac{\tau^{\epsilon_{i}}}{\displaystyle \sum_{j=1}^{9} \tau^{\epsilon_{j}}}
\label{modified bolztman}
\end{align}

\textbf{If terminal state:}
\begin{align}
    Q(s_{t}, a) \leftarrow (1-\alpha) \cdot Q(s_{t}, a) + \alpha r
\label{terminal state update}
\end{align}

\textbf{If non terminal state: }
\begin{align}
    Q(s_{t}, a) \leftarrow (1-\alpha) \cdot Q(s_{t}, a) + \alpha \max_{a}{Q(s_{t + 1}, a)} 
\label{non terminal state update}
\end{align}

As agents learn their drawrate or winrate depending on if they are playing against a perfect or random opponent increases in the form of the decaying exponential. As \textbf{X} learns against optimal \textbf{O} the draws increase in the form of a decaying exponential draws \% = $e^{-k \cdot \text{games played}}$ as seen in Figure \ref{fig:x_vs_perfect_distribution_1}.  

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{"../../Part 1: Tic Tac Toe/figures/x_vs_perfect_distribution_1.png"} % Path to your image
    \caption{\textbf{X} training against perfect \textbf{O} causes the drawrate in the form of a decaying exponential}
    \label{fig:x_vs_perfect_distribution_1}
\end{figure}

\section{Variations in $\alpha$ and $\tau$}
The values of hyperparameters $\tau$ and $\alpha$  matter significantly. Different models for $\tau$ and $\alpha$ have been tested and two models are proposed as best candidates as seen in 

\begin{align}
    \alpha(\text{turn}, a) = \alpha_{inital} \cdot \frac{1}{1 + a \cdot \text{turn}}\\
    \tau(\text{turn}, \text{total games}, t) = 1 + (t - 1) \cdot (\frac{\text{turn}}{\text{total games}})^{3}
\end{align}

In order to pick out the best parameters $a$ and $t$ for $\alpha$ and $\tau$ respectively, a range of $a$ and $t$ was picked and an agent was trained for each combination as seen in Figure \ref{x_vs_perfect_range_of_params_1.png} (This is actually a wrong figure, the right figure is generating overnight (cause training takes a while) but the plot would be similar).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{"../../Part 1: Tic Tac Toe/figures/x_vs_perfect_range_of_params_1.png"} % Path to your image
    \caption{After training \textbf{X} on $10000$ batches of $10$ games, the highest performing values for parameters $a$ and $t$ are $0.00000001$ and $10^{6}$ respectively}
    \label{x_vs_perfect_range_of_params_1.png}
\end{figure}


\section{Optimal and non optimal opponent}

\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{2} % Extra height for your RL board states
    \begin{tabular}{|l|c|c|}
        % 1. Draw the top horizontal line ONLY for columns 2 and 3
        \cline{1-3}
        
        % 2. Use multicolumn to remove the left-most border of the first cell
        \textbf{Agent} & \text{\textbf{X} trained on perfect} & \text{\textbf{X} trained on random} \\ \hline
        
        \text{\textbf{O} trained on perfect} & \makecell{\textbf{X} winrate: \\ drawrate: \\ \textbf{O} winrate:} & \textbf{O} winrate: \\ \hline
        
        \text{\textbf{O} trained on random} & Value 2,1 & Value 2,2 \\ \hline
    \end{tabular}
    \caption{Matrix of results of agents trained on different opponents playing against each other.}
    \label{tab:3x3_example}
\end{table}

\section{Theoretical Q-Table compared to RL Q-Table}

\chapter{Deep Q-Learning}
\textbf{Note:} AI was slightly used for the creating of the library, specifically it helped to create the function handling the backpropogation and the function handling the loss function\\\\

\noindent
\textbf{Note:} Although the library was mostly self made, in order to quickly test if it works (instead of spending a lot of time writing a game myself only to discover I have made an error with my neural network structure), I have made AI implement my library using 2 games made by Open-AI for training agents, ``cart pole'' and ``lunar lander''\\\

The main disadvantage of traditional Q-Learning is that it only applies to discrete states. If the input is continuous, then it will no longer work, as the Q-Table would be infinitely large. This problem is solved by Deep Q-Learning, instead of a Q-Table giving discrete q-values for actions, a neural network predicts the q-values instead.\\\\

A simple library involving numpy arrays was made in order to allow deep q-learning training.
\section{Neural Network and the loss fucntion}

\begin{align}
    \text{Loss function logic goes here}
\end{align}

\begin{align}
    \text{Backpropogation matrix calculus goes here}
\end{align}

\section{cart-pole game}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{"../../Part 2: Deep Q-Learning/fgiures/CartPole-v1_example.png"} % Path to your image
    \caption{The library I made is able to train a neural network to control a cart balancing a pole. The neural network takes in 4 parameters and generates 2 outputs.}
    \label{fig:CartPole-v1_example.png}
\end{figure}

\section{moon-landing game}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{"../../Part 2: Deep Q-Learning/fgiures/LunarLander-v3_example.png"} % Path to your image
    \caption{The library I made is able to train a neural network to control a space ship landing on the moon. The neural network takes in 8 parameters and generates 4 outputs.}
    \label{fig:LunarLander-v3_example.png}
\end{figure}

\chapter{Conclusion}

\chapter{Bibliography}
\end{document}
