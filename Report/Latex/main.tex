\documentclass{report}
\usepackage{amsmath, amssymb, amsthm} % Essential math tools
\usepackage[table]{xcolor}
\usepackage[hidelinks]{hyperref} % hidelinks removes the ugly red/green boxes around links
\usepackage{cleveref}   
\usepackage[utf8]{inputenc}
\usepackage{makecell} % Required for multi-line cells
\usepackage{comment}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{float}

\usepackage{bm}

\usepackage[british]{babel}
\usepackage[authoryear]{natbib} % Handles the (Author, Year) style

\lstset{
    basicstyle=\ttfamily\small,        % Use typewriter font, small size
    breaklines=true,                   % !!! This fixes code running off the page
    breakatwhitespace=true,            % Only break lines at spaces
    columns=fullflexible,              % !!! This fixes the weird character spacing
    keepspaces=true,                   % Keeps indentation spaces
    numbers=left,                      % Line numbers on the left
    numberstyle=\tiny\color{gray},     % Style of line numbers
    frame=single,                      % Adds a box around the code
    rulecolor=\color{black},           % Color of the box
    commentstyle=\color{green!50!black}, % Color of comments
    keywordstyle=\color{blue},         % Color of keywords
    showstringspaces=false,            % Don't show dots for spaces
    xleftmargin=15pt,                  % Indent the whole block slightly
}

% This defines a "Pseudocode" style for listings
\lstdefinelanguage{Pseudocode}{
    morekeywords={for, in, if, else, for each, while, return, end},
    sensitive=false,
    morecomment=[l]{//},
    morestring=[b]",
}

\title{\textbf{Q-Learning}}
\author{Peter (Petr) Ladur\\\\56445760\\\\Supervised by: Elena Moltchanova}
\date{}

\begin{document}
\maketitle
\chapter*{Acknowledgements}
I would like to express my gratitude to my supervisor Elena Moltchanova for their guidance and technical feedback throughout this summer research project. \\\\

\noindent
I would also like to thank the Mathematics Department at UC for allowing me to undertake this summer research project.

\chapter{Introduction}
Games are often the perfect sandbox for experimenting with machine learning algorithms. The aim of this project is to explore Q-Learning in the context of tic-tac-toe and other applications using Python. Since tic-tac-toe is a drawn game assuming perfect play and involves no chance, there is a clear baseline for how well a random agent would perform. Theoretical Q-Table was generated using the ``minimax'' algorithm, and was compared to reinforcement learning (RL) estimated Q-Tables through training against both, random and perfect opponent. To analyse different training policies (perfect/random opponent), results of agents trained on different policies playing against each other were recorded. Hyperparameters significantly affect the training results, so a range of them were explored and functions for $\alpha$ and $\tau$ were proposed. Lastly, in order to analyse more complex games, Deep Q-Learning was investigated in the context of different games in the Open-AI's ``gymnasium''.

\chapter{Background}
\section{Q-Table}
Q-Table is a table which contains the total expected reward after every possible action in a particular state assuming the optimal policy is followed. In the context of tic-tac-toe the state is the game position at that particular moment, and actions are all the empty squares. As seen in \cref{tab:perfect_Q_Table_entry}, for that particular state there are four already occupied positions, two actions that lead to a total expected reward of $1$ (wins) and three actions which lead to a total expected reward of $0$ (draws) assuming perfect play. The Q-Table can then be used to select moves which maximise the total reward, creating a perfect tic-tac-toe bot.

%Q-Table example
\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
% Draws top line starting from 2nd column
\cline{2-10}
\multicolumn{1}{c|}{} & (1, 1) & (1, 2) & (1, 3) & (2, 1) & (2, 2) & (2, 3) & (3, 1) & (3, 2) & (3, 3) \\ \hline
\makecell{\_ \_ o \\ \_ x \_ \\ x o \_}  & 
% This is the "No-Package" multi-line trick:
\cellcolor{black!60}& 
1 & 1 & \cellcolor{black!60} & \cellcolor{black!60} & 0 & 0 & 0 & \cellcolor{black!60}\\ \hline
\end{tabular}
% 3. ADD THE CAPTION AND LABEL HERE
\caption{Two actions lead to a win, three actions lead to a draw assuming perfect play from both sides}
\label{tab:perfect_Q_Table_entry} 
\end{table}
 
\section{Q-Learning}
In the context of Q-Learning, the Q-Table can be filled up by playing games and updating the values of actions taken during that game according to the Bellman equation. 

\begin{align}
Q(s, a) \leftarrow (1-\alpha) \cdot Q(s, a) + \alpha (r + \gamma \cdot \max_{a'}{Q(s', a')}) 
\end{align}

Where:
\begin{itemize}
    \item $Q(s, a)$ is the predicted outcome for the action at a particular state $s$
    \item $s'$ and $a'$ are the future state and action respectively
    \item $0 \leq \alpha \leq 1$  is the learning rate
    \item $0 \leq \gamma \leq 1$ is the discount factor
    \item $r$ is the immediate reward for achieving a state
\end{itemize}

The learning rate $\alpha$ determines how much does the new information replace the already known information. A learning rate of $1$ would mean that each training iteration the $Q(s, a)$ is completely replaced by $(r + \gamma \cdot \max_{a'}{Q(s', a')})$. A learning rate of $0$ would imply that $Q(s, a)$ is not affected by training at all. The learning rate can be adjusted during training as the model becomes more stable.\\

The immediate reward $r$ in the context of tic-tac-toe is $0$ if the game is not terminated. If the game is terminated, the immediate reward is $1$ for a game that is won by \textbf{X}, $0$ for a game that is drawn and $-1$ for a game that is won by \textbf{O}. \\

The discount factor is a measure of how much the future is important compared to the current decision. For example in the context of tic-tac-toe the future matters just as much as the current situation, no matter if you lose on move 5 or move 7, you still lose. Meanwhile, in a long game with no set win condition, such as a player trying to dodge obstacles, often the most important thing is not to die right now, the future is less important.


\chapter{Q-Learning on tic-tac-toe}

\section{Generating theoretical Q-Table}
The ``minimax'' algorithm can be used in turn-based games to calculate the best policy. It is designed to minimise the potential loss and maximise the potential gain. The algorithm works by first generating all the possible final states and then taking turns ``undoing'' the moves that could have lead to that state by removing \textbf{X}'s and \textbf{O}'s until the initial state (empty board) is reached. Each time an \textbf{X} or \textbf{O} is removed the worst case scenario is considered as seen in code box \ref{lst:minimax}. This explores the game trees and backproagates the values, filling up a theoretical Q-Table.

\begin{lstlisting}[caption={Minimax algorithm for generating the theoretical Q-Table}, label={lst:minimax}, language=Pseudocode]

for parent_state in parent_states:
    
    for x/o in parent_state:

        //"undoing" a move from a state
        child_state = parent_state with an x/o removed

        if child_state is terminal_state:
            Q_Table[child_state] = [game_result] * 9
        else:
            //min/max depends on whether x/o removed
            Q_Table[child_state][index of x/o] = min/max(Q_Table[parent_state])  

            parent_states = child states
\end{lstlisting}



\section{Q-Learning implementation}

During the Q-Learning algorithm batches of games were played. The agent would choose moves using a weighted probability values $p$ according to a slightly modified Boltzmann function as seen in \eqref{modified Boltzmann}. States $s$, actions taken $a$ and results $r$ are stored in a queue. The queue is then emptied and Q-Table is updated using the Bellman equation as seen in \eqref{terminal state update} and \eqref{non terminal state update}.\\\\

\begin{align}
    p_{i}(\tau) = \frac{\tau^{\epsilon_{i}}}{\displaystyle \sum_{j=1}^{9} \tau^{\epsilon_{j}}}
\label{modified Boltzmann}
\end{align}

Where $\epsilon_{i} = Q(s, a_{i})$\\

\textbf{If terminal state:}
\begin{align}
    Q(s, a) \leftarrow (1-\alpha) \cdot Q(s, a) + \alpha r
\label{terminal state update}
\end{align}

\textbf{If non-terminal state: }
\begin{align}
    Q(s, a) \leftarrow (1-\alpha) \cdot Q(s, a) + \alpha \max_{a'}{Q(s', a')} 
\label{non terminal state update}
\end{align}

As agents train, their draw rate or win rate increases in the form of the decaying exponential depending on the type of opponent. As \textbf{X} learns against optimal \textbf{O} the draws increase in the form of a decaying exponential as seen in \cref{fig:x_vs_perfect_distribution_2}. Meanwhile, as  \textbf{X} learns against a random \textbf{O} the proportion of \textbf{X} wins increases exponentially as seen in \cref{fig:x_vs_random_distribution_1}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{"../../Part 1: Tic Tac Toe/figures/x_vs_perfect_distribution_2.png"} % Path to your image
    \caption{\textbf{X} training against perfect \textbf{O} causes the draw rate to increase in the form of a decaying exponential}
    \label{fig:x_vs_perfect_distribution_2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{"../../Part 1: Tic Tac Toe/figures/x_vs_random_distribution_1.png"} % Path to your image
    \caption{\textbf{X} training against random \textbf{O} causes the \textbf{X} win rate to increase in the form of a decaying exponential}
    \label{fig:x_vs_random_distribution_1}
\end{figure}

\section{Variations in $\alpha$ and $\tau$}
The values of hyperparameters $\tau$ and $\alpha$  matter significantly. During the game it is best to decrease the learning rate. Initially it is desired for the new information to rapidly change the Q-Table, but eventually a low learning rate means that one bad game will not significantly affect the Q-values trained over hundreds of games in the past. Meanwhile, $\tau$ should increase switching from exploration to exploitation. Different models for $\tau$ and $\alpha$ have been tested, and two models are proposed are seen in equations \eqref{eq:alpha_model} and \eqref{eq:tau_model}.

\begin{equation}
    \alpha(\text{decay rate}) = \alpha_{initial} \cdot e^{-\text{decay rate} \times \text{game number}}\\
    \label{eq:alpha_model}
\end{equation}
\begin{equation}
    \tau(\text{growth rate}) = \text{Maximum Rate}^{((\frac{\text{game number}}{\text{total games}})^{\text{growth rate}})}
    \label{eq:tau_model}
\end{equation}

In order to pick out the best parameters ``\textit{growth rate}'' and ``\textit{decay rate}'' for $\alpha$ and $\tau$ respectively, a range of ``\textit{growth rates}'' and ``\textit{decay rates}'' was picked, and an agent was trained for each combination as seen in \cref{x_vs_perfect_range_of_params_1.png}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{"../../Part 1: Tic Tac Toe/figures/x_vs_perfect_range_of_params_1.png"} % Path to your image
    \caption{After training \textbf{X} on $1000$ batches of $10$ games against a perfect opponent, the highest performing values for parameters \textit{growth rate} are $10^{\frac{-1}{3}}$ and \textit{decay rate} between $10^{-4}$ and $10^{-10}$}
    \label{x_vs_perfect_range_of_params_1.png}
\end{figure}

\begin{align*}
    \alpha(\text{decay rate}) = \alpha_{initial} \cdot e^{-10^{-7} \times \text{game number}}\\
    \tau(\text{growth rate}) = \text{1000}^{((\frac{\text{game number}}{\text{total games}})^{10^{\frac{-1}{3}}})}
\end{align*}

\section{Optimal and non-optimal opponent}

After training agents on different type of opponents, I have made them play 10000 games against each other collecting results in \cref{tab:matrix of agents playing against each other}. 

\begin{itemize}
    \item An agent trained on a random policy does very well against an agent trained on a perfect policy. The reason for this, is that an agent trained on a perfect policy can only choose the moves at random if its opponent makes a mistake. For example \textbf{X} trained on perfect would have never been exposed to a state where it can win, so it will not know where to go and will make a random move.
    \item \textbf{X} and \textbf{O} trained on a perfect opponent perform very close to a random performance in a match against each other. The likely reason for this, is that both of the opponents have been trained to draw and have not encountered the states where a non-optimal move is played.
    \item The likely reason \textbf{X} and \textbf{O} wins for agents trained on a random policy, is that they expect their opponent to continue playing the game randomly. Sometimes they might go for ``risky'' plays which would have a high probability of a winning/drawing against a random opponent, but encounter resistance against an actually trained opponent.
\end{itemize}

\begin{table}[H]
    \centering
    \renewcommand{\arraystretch}{3} % Extra height for your RL board states
    \begin{tabular}{|l|c|c|}
        % 1. Draw the top horizontal line ONLY for columns 2 and 3
        \cline{1-3}
        
        % 2. Use multicolumn to remove the left-most border of the first cell
        \textbf{Agent} & \text{\textbf{X} trained on perfect} & \text{\textbf{X} trained on random} \\ \hline
        
        \text{\textbf{O} trained on perfect} & \makecell{\textbf{X} win rate: 58.89\% \\ draw rate: 14.12\%\\ \textbf{O} win rate:  26.99\%} & \makecell{\textbf{X} win rate: 86.05\% \\ draw rate: 8.36\%\\ \textbf{O} win rate: 5.59\%}\\ \hline
        
        \text{\textbf{O} trained on random} & \makecell{\textbf{X} win rate: 15.73\% \\ draw rate: 23.24\%\\ \textbf{O} win rate: 61.08\%} & \makecell{\textbf{X} win rate: 43.31\% \\ draw rate: 30.02\%\\ \textbf{O} win rate: 26.67\%} \\ \hline
    \end{tabular}
    \caption{Matrix of results of agents trained on different opponents playing against each other.}
    \label{tab:matrix of agents playing against each other}
\end{table}



\section{Theoretical Q-Table compared to RL-estimated Q-Table}

The theoretical Q-Table generated using the minimax algorithm has differences to the Q-Table generated through RL. \\\\

When generating the Q-Table of \textbf{X} playing against a perfect opponent, \textbf{O} will never end up in a losing position. Hence, states where \textbf{O} has a chance to lose are never explored and much of the RL generated Q-Table is unfilled. For the states which are explored the values of the RL generated Q-Table are similar to theoretical values as seen in \cref{tab:Q-Table comparison for ____o_oxx} \\\\

The perfect Q-Table differs to the RL-estimated Q-Table against a random opponent. When generating the Q-Table of \textbf{X} against a random opponent, more states are explored, but the future policy is different. In the state seen in \cref{tab:Q-Table comparison for _______ox} two out of three actions which lead to a guaranteed win (assuming perfect play), were given the Q-values close to 1. Likely, the reason why action (2, 3) was given the value of 0.00 is because it wasn't explored enough during the exploration phase.


%Q-Table example
\begin{table}[H]
\centering
\renewcommand{\arraystretch}{2.5}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
% Draws top line starting from 2nd column
\cline{1-10}
\multicolumn{1}{|c|}{\makecell{\_ \_ \_ \\ \_ o \_ \\ o x x}} & (1, 1) & (1, 2) & (1, 3) & (2, 1) & (2, 2) & (2, 3) & (3, 1) & (3, 2) & (3, 3) \\ \hline
\makecell{Theoretical Q-Table \\(perfect opponent)}& \cellcolor{black!60}& \cellcolor{black!60}  & \cellcolor{black!60}  & -1 & \cellcolor{black!60} & -1 & -1 & -1 & 0\\ \hline
RL-estimated Q-Table & \cellcolor{black!60}& \cellcolor{black!60} & \cellcolor{black!60} & -0.75 & \cellcolor{black!60} & -0.75 & -0.75 & -0.66 & 0\\ \hline
\end{tabular}
% 3. ADD THE CAPTION AND LABEL HERE
\caption{The theoretical and RL-estimated Q-Table for x playing against perfect opponent, give similar Q-values for explored states}
\label{tab:Q-Table comparison for ____o_oxx} 
\end{table}

%Q-Table example
\begin{table}[H]
\centering
\renewcommand{\arraystretch}{2.5}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
% Draws top line starting from 2nd column
\cline{1-10}
\multicolumn{1}{|c|}{\makecell{\_ \_ \_ \\ \_ \_ \_ \\ \_ o x}} & (1, 1) & (1, 2) & (1, 3) & (2, 1) & (2, 2) & (2, 3) & (3, 1) & (3, 2) & (3, 3) \\ \hline
\makecell{Theoretical Q-Table \\(perfect opponent)}& 0 & \cellcolor{black!60} & \cellcolor{black!60} & 0 & 1 & 1  & 0 & 0 & 1\\ \hline
RL-estimated Q-Table & 0.062 & \cellcolor{black!60} & \cellcolor{black!60} & 0.00 & 1.00 & 0.00 & 0.06 & 0.02 & 0.83 \\ \hline
\end{tabular}
% 3. ADD THE CAPTION AND LABEL HERE
\caption{The theoretical Q-Table for x playing against a random opponent, and RL-estimated Q-Table for x playing against a random opponent give different values, because the future policy is different}
\label{tab:Q-Table comparison for _______ox} 
\end{table}



\chapter{Deep Q-Learning}
The main disadvantage of traditional Q-Learning is that it only works well with a few discrete states. If the input is continuous, then it will no longer work, as the Q-Table would need to be infinitely large. This problem is solved by implementing a Deep Q-Network (DQN), instead of a Q-Table. While in a Q-Table a state corresponds to all the exact Q-values which can be taken by different actions, DQN estimates the Q-values for all actions instead \citep{geeksforgeeks_2019_deep}.\\

I have programmed a simple NumPy-based library in Python which allows for an implementation of a DQN.
\section{Neural Network structure, Backpropagation, Cost Function}

By structure DQN is identical to a normal neural network. It takes in the state such as coordinates or stock prices in the input layer and forward propagates the input through the network to return the estimated Q-values for different actions in the output layer as seen in equation \eqref{eq:forward propagation}. 

\begin{align}
    \mathbf{a}^{(l+1)} = \sigma \left( \mathbf{W}^{(l)} \mathbf{a}^{(l)} + \mathbf{b}^{(l)} \right)
    \label{eq:forward propagation}
\end{align}

The main difference between a Q-Table and a Q-Network is while during training a Q-Table, the Q-values are simply updated using the Bellman equation, in a DQN weights and biases are adjusted. With a converged Q-Table $Q(s, a) = r + \gamma \max_{a'} Q(s', a')$ and very similarly, with a trained Deep Q-Network $Q(s, a, \theta) = r + \gamma \max_{a'} Q(s', a', \theta)$. During DQN training, an action picked is compared to the target as seen in equations \eqref{eq:target} and \eqref{eq:loss function}. The weights and biases $\theta$ are then adjusted through backpropagation with respect to the cost function $\mathcal{C}(\bm{\theta})$ as seen in code box \ref{lst:backprop}. When calculating the target, a copy of the DQN with constant parameters $\theta^{-}$ is used which are updated to match the parameters $\theta$ every N steps \citep{mnih_2015_humanlevel}. 

\begin{equation}
    \text{target} = r + \gamma \max_{a'} Q(s', a'; \bm{\theta}^-)
    \label{eq:target}
\end{equation}

\begin{equation}
    \mathcal{C}(\bm{\theta}) = \mathbb{E}_{(s, a, r, s')} \left[\frac{1}{2} \left ( Q(s, a; \bm{\theta}) - \text{target}) \right)^2 \right]
        \label{eq:loss function}
\end{equation}

\begin{lstlisting}[caption={Backpropagation implementation for Q-Learning}, label={lst:backprop}, language=Python]

dZ[actions, batch_indices] = predictions - targets #dC/dA

#backpropagate through the layers until we get to the input layer
for i in range(len(weights) -1, -1, -1):

    A_prev = forward_propagation_params_A[i]  #previous activation layer

    dW = 1/m * dZ @ A_prev.T #dC/dW = dC/dA * dA/dW
    db = 1/m * np.sum(dZ, axis=1, keepdims=True) #dC/db = dC/dA * dA/db

    dW_list.insert(0, dW)
    db_list.insert(0, db)


    if i > 0:
        #this is the dA/dZ = f'(Z) * dZ
        dZ = (weights[i].T @ dZ) * 
        functions_deriv[i - 1](forward_propagation_params_Z[i - 1])   
\end{lstlisting}

\section{Deep Q-Learning Results}
Having made a simple library for implementing a DQN, with the help of AI, I have implemented it in the context of two simple games.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{"../../Part 2: Deep Q-Learning/figures/CartPole-v1_example.png"} % Path to your image
    \caption{The library I made is able to train a neural network to control a cart balancing a pole. The neural network takes in 4 parameters and generates 2 Q-values.}
    \label{fig:CartPole-v1_example.png}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{"../../Part 2: Deep Q-Learning/figures/LunarLander-v3_example.png"} % Path to your image
    \caption{The library I made is able to train a neural network to control a spaceship landing on the moon. The neural network takes in 8 parameters and generates 4 Q-values.}
    \label{fig:LunarLander-v3_example.png}
\end{figure}
\chapter{Conclusion}
In this project I have succeeded in implementing traditional Q-Learning in the context of tic-tac-toe. With help of AI, I implemented Deep Q-Learning in the context of two simple games. During Q-Learning in tic-tac-toe agents learn in an exponential manner against perfect and a random opponent drawing and winning more respectively. The most notable feature discovered when making agents trained on different policies play against each other, was that agents trained on the random policy would outperform agents trained on the perfect policy. The RL-estimated Q-Table was compared to a theoretical Q-Table was similar during training against a perfect opponent but was different against a random opponent due to differences in future policy. During the Deep Q-Learning implementation, a small NumPy-based library was programmed, which could then be applied to various environments, such as the ``Cart Pole'' and ``Lunar Lander'' games developed by Open-AI.

\chapter{Future Work}

During this project there were several areas which I would like to explore further.\\

\begin{itemize}
    \item Experiment with other hyperparameter functions such as piecewise functions. Particularly, to only explore initially, and only exploiting way later in the game.
    \item Experiment with the discount factor $\gamma$. Although it doesn't matter if you win on move $5$ or move $7$, winning faster is preferred, as it is still possible for an agent to make a mistake in the future
    \item Implementing a DQN in the context of a stock market. I would like to input historical data into a DQN and train it to sell, buy or hold, trying to make profit
\end{itemize}


\chapter{Appendices}

\section{AI Declaration}
Gemini AI was slightly used during coding, mainly for helper functions. In the context of the DQN library, it specifically helped to create the function handling the backpropagation and the function handling the cost.\\


Although the libraries are mostly self-made, in order to quickly test if the DQN library works, I have made Gemini AI implement it using two games made by Open-AI for training agents, ``cart pole'' and ``lunar lander''\\

Gemini AI was used for report polishing and extensively for LaTex formatting.
\section{Code}

All the code for this project is accessible in a public GitHub repository. It can be accessed via this link:\\ \url{https://github.com/peterladur/Reinforcement-Learning-Research-Project}

\begin{thebibliography}{99}

\bibitem[Evolutionary Intelligence(2021)]{intelligence_2021_reinforcement}
Evolutionary Intelligence. (2021, April). \textit{Reinforcement learning: Tic-tac-toe} [Video]. YouTube. \url{https://www.youtube.com/watch?v=0_u66_u8-mY}

\bibitem[GeeksforGeeks(2019a)]{geeksforgeeks_2019_qlearning}
GeeksforGeeks. (2019, February). \textit{Q-Learning in reinforcement learning}. \url{https://www.geeksforgeeks.org/machine-learning/q-learning-in-python/}

\bibitem[GeeksforGeeks(2019b)]{geeksforgeeks_2019_deep}
GeeksforGeeks. (2019, June). \textit{Deep Q-Learning in reinforcement learning}. \url{https://www.geeksforgeeks.org/deep-learning/deep-q-learning/}

\bibitem[Mnih et al.(2015)]{mnih_2015_humanlevel}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, Joel., Bellemare, M. G., ... \& Hassabis, D. (2015). Human-level control through deep reinforcement learning. \textit{Nature}, \textit{518}(7540), 529--533. \url{https://doi.org/10.1038/nature14236}

\bibitem[Zhang(2020)]{zhang_2020_building}
Zhang, S. (2020, November). \textit{Building a neural network FROM SCRATCH (no Tensorflow/Pytorch, just numpy \& math)} [Video]. YouTube. \url{https://www.youtube.com/watch?v=w8yvEqnraU8}

\end{thebibliography}
\end{document}
