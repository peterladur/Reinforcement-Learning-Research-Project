\documentclass{report}
\usepackage{amsmath, amssymb, amsthm} % Essential math tools
\usepackage[table]{xcolor}
\usepackage{cleveref}   
\usepackage[utf8]{inputenc}
\usepackage{makecell} % Required for multi-line cells\
\usepackage{comment}
\usepackage{listings}
\usepackage[draft]{graphicx}
\usepackage{coffeestains}
\usepackage{bm}

\title{\textbf{Q-Learning}}
\author{Peter (Petr) Ladur\\\\56445760}
\date{}


\begin{document}
\maketitle
\chapter{Introduction}
\coffeestainC{0.9}{0.85}{-25}{5cm}{1.3cm} 
The aim of this project is to explore Q-Learning in the context of tic-tac-toe and other applications using python. Theoretical Q-Table was generated using the ``minimax'' algorithm, and was compared to Q-Tables generated through training against both, random and perfect opponent. Results of agents trained on different policies playing against each other were recorded. How hyperparameters ($\alpha$, $\tau$) affect training results was investigated and an optimal function for hyperparameters was proposed. Deep-Q-Learning was investigated in the context of different games in the ``Open-AI gymnasium'' (\textit{I know that this is unlikely but if I finish the report and still have some time left over, I will try playing around with another application of Deep Q-Learning such as the stock market}). 

\chapter{Background}
\section{Q-Table}
Q-Table is a table which contains contains the total expected reward after every possible action in a particular state assuming the optimal policy is followed. In the context of tic-tac-toe the state is the game position at that particular moment, and actions are all the empty squares. As seen in \cref{tab:perfect_Q_Table_entry}, for that particualr state there are two actions that lead to a total expected reward of 1 (wins) and three actions which lead to a total expected reward of 0 (draws) assuming perfect play. The Q-Table can then be used to select moves which maximise the total reward. 

%Q-Table example
\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
% Draws top line starting from 2nd column
\cline{2-10}
\multicolumn{1}{c|}{} & (1, 1) & (1, 2) & (1, 3) & (2, 1) & (2, 2) & (2, 3) & (3, 1) & (3, 2) & (3, 3) \\ \hline
\makecell{\_ \_ o \\ \_ x \_ \\ x o \_}  & 
% This is the "No-Package" multi-line trick:
\cellcolor{black!60}& 
1 & 1 & \cellcolor{black!60} & \cellcolor{black!60} & 0 & 0 & 0 & \cellcolor{black!60}\\ \hline
\end{tabular}
% 3. ADD THE CAPTION AND LABEL HERE
\caption{Two actions lead to a win, three actions lead to a draw assuming perfect play from both sides}
\label{tab:perfect_Q_Table_entry} 
\end{table}
 
\section{Q-Learning}
In the context of Q-Learning, the Q-Table can be filled up by playing games and updating the values of actions taken during that game according to the Bellman equation. 

\begin{align}
Q(s_{t}, a) \leftarrow (1-\alpha) \cdot Q(s_{t}, a) + \alpha (r + \gamma \cdot \max_{a}{Q(s_{t + 1}, a)}) 
\end{align}

Where:
\begin{itemize}
    \item $Q(s, a)$ is the predicted outcome for the action at a particular state $s$
    \item $0 \leq \alpha \leq 1$  is the learning rate
    \item $0 \leq \gamma \leq 1$ is the discount factor
    \item $r$ is the immidiate reward for achiveing a state
\end{itemize}

The learning rate $\alpha$ determines how much does the new information replace the already known information. A learning rate of $1$ would mean that each training iteration the $Q(s, a)$ is completely replaced by $(r + \gamma \cdot \max_{a}{Q(s_{t + 1}, a)})$. A learning rate of $0$ would imply that $Q(s, a)$ is not affected by training at all. The learning rate can be adjusted during training as the model becomes more stable.\\

The immidiate reward in the context of tic-tac-toe is $1$ for a game that is won by \text{X}, $0$ for a game that is drawn and $-1$ for a game that is won by \text{O}.

The discount factor is a measure of how much the future is important compared to the current decision. For example in the context of tic-tac-toe the future matters just as much as the the current situation, no matter if you loose on move 5 or move 7, you still loose. Meanwhile in a long game with no set win condition, such as a player trying to dodge obstacles, often the most important thing is not to die right now, the future is less important.




\chapter{Q-Learning on tic-tac-toe}

\section{Generating theoretical Q-Table}
The ``minimax'' algorithm can be used in turn-based games to calculate the best policy. It is designed to minimise the potential loss and maximise the potential gain. The algorithm works by first generating all the possible final states and then taking turns ``undoing'' the moves that could have lead to that state by removing \textbf{X}'s and \textbf{O}'s until the initial state (empty board) is reached. Each time an \textbf{X} or \textbf{O} is removed the worst case scenario is considered, filling up the theoretical Q-Table.

\begin{lstlisting}
terminal_states = generate_all_terminal_states()

for parent_state in parent_states:
    
    for x/o in parent_state:

        child_state = parent_state with an x/o removed

        if child_state is terminal_state:
            Q_Table[child_state] = [game_result] * 9
        else:
            #min or max depends on whether it's x/o removed respectively 
            Q_Table[child_state][index of x/o] = min/max(Q_Table[parent_state])  

            parent_states = child states
\end{lstlisting}

\section{Q-Learning implementation}

During the Q-Learning algorihtm batches of games were played. The agent would choose moves using a weighted probability values according to a slightly modified bolztman function as seen in \eqref{modified bolztman}. States $s$, actions taken $a$ and results $r$ are stored in a queue. The queue is then emptied and Q-Table is updated using the Bellman equation as seen in \eqref{terminal state update} and \eqref{non terminal state update}.\\\\

\begin{align}
    p_{i}(\tau) = \frac{\tau^{\epsilon_{i}}}{\displaystyle \sum_{j=1}^{9} \tau^{\epsilon_{j}}}
\label{modified bolztman}
\end{align}

\textbf{If terminal state:}
\begin{align}
    Q(s_{t}, a) \leftarrow (1-\alpha) \cdot Q(s_{t}, a) + \alpha r
\label{terminal state update}
\end{align}

\textbf{If non terminal state: }
\begin{align}
    Q(s_{t}, a) \leftarrow (1-\alpha) \cdot Q(s_{t}, a) + \alpha \max_{a}{Q(s_{t + 1}, a)} 
\label{non terminal state update}
\end{align}

As agents learn their drawrate or winrate depending on if they are playing against a perfect or random opponent increases in the form of the decaying exponential. As \textbf{X} learns against optimal \textbf{O} the draws increase in the form of a decaying exponential draws \% = $e^{-k \cdot \text{games played}}$ as seen in Figures \ref{fig:x_vs_perfect_distribution_2}. As \textbf{X} learns against a random \textbf{O} the proportion of \textbf{X} wins increases exponentially as seen in Figure \ref{fig:x_vs_random_distribution_1}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{"../../Part 1: Tic Tac Toe/figures/x_vs_perfect_distribution_2.png"} % Path to your image
    \caption{\textbf{X} training against perfect \textbf{O} causes the drawrate in the form of a decaying exponential}
    \label{fig:x_vs_perfect_distribution_2}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{"../../Part 1: Tic Tac Toe/figures/x_vs_random_distribution_1.png"} % Path to your image
    \caption{\textbf{X} training against perfect \textbf{O} causes the drawrate in the form of a decaying exponential}
    \label{fig:x_vs_random_distribution_1}
\end{figure}

\section{Variations in $\alpha$ and $\tau$}
The values of hyperparameters $\tau$ and $\alpha$  matter significantly. Different models for $\tau$ and $\alpha$ have been tested and two models are proposed as best candidates as seen in 

\begin{align}
    \alpha(\text{decay rate}) = \alpha_{inital} \cdot e^{-\text{decay rate} \times \text{game number}}\\
    \tau(\text{growth rate}) = \text{Maximum Rate}^{((\frac{\text{game number}}{\text{total games}})^{\text{growth rate}})}
\end{align}

In order to pick out the best parameters $a$ and $t$ for $\alpha$ and $\tau$ respectively, a range of $a$ and $t$ was picked and an agent was trained for each combination as seen in Figure \ref{x_vs_perfect_range_of_params_1.png} (This is actually a wrong figure, the right figure is generating overnight (cause training takes a while) but the plot would be similar).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{"../../Part 1: Tic Tac Toe/figures/x_vs_perfect_range_of_params_1.png"} % Path to your image
    \caption{After training \textbf{X} on $1000$ batches of $10$ games, the highest performing values for parameters \textit{growth rate} are $10^{\frac{-1}{3}}$ and \textit{decay rate} between $10^{-4}$ and $10^{-10}$  respectively}.
    \label{x_vs_perfect_range_of_params_1.png}
\end{figure}

\begin{align}
    \alpha(\text{decay rate}) = \alpha_{inital} \cdot e^{-10^{-7} \times \text{game number}}\\
    \tau(\text{growth rate}) = \text{1000}^{((\frac{\text{game number}}{\text{total games}})^{10^{\frac{-1}{3}}})}
\end{align}

\section{Optimal and non optimal opponent}

After training different agents, I have made them play 10000 against each other collecting results in Table \ref{tab:matrix of agents playing against each other}. 

\begin{itemize}
    \item An agent trained against the random policy does very well against an agent trained on an optimal policy. The reason for this, is that an agent trained on the perfect policy can only choose the moves at random if it's opponent makes a mistake. For example \textbf{X} trained on perfect would never have been exposed to a state where it can win, so it will not know where to go and will make a random move.
    \item \textbf{X} and \textbf{O} trained on a perfect opponent perform very close to a random performance in a match against each other. The likely reason for this, is that both of the opponents have been trained to draw and have not encountered the states where a non-optimal move is played.
    \item The likely reason for high \textbf{X} and \textbf{O} wins for agaents trained on a random policy, is that they expect their opponent to continue playing the game randomly. Sometimes they might go for ``risky'' plays which would have a high probability of a win/draw against a random opponent, but encounter resistance against an actually trained opponent.
\end{itemize}

\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{3} % Extra height for your RL board states
    \begin{tabular}{|l|c|c|}
        % 1. Draw the top horizontal line ONLY for columns 2 and 3
        \cline{1-3}
        
        % 2. Use multicolumn to remove the left-most border of the first cell
        \textbf{Agent} & \text{\textbf{X} trained on perfect} & \text{\textbf{X} trained on random} \\ \hline
        
        \text{\textbf{O} trained on perfect} & \makecell{\textbf{X} winrate: 58.89\% \\ drawrate: 14.12\%\\ \textbf{O} winrate:  26.99\%} & \makecell{\textbf{X} winrate: 86.05\% \\ drawrate: 8.36\%\\ \textbf{O} winrate: 5.59\%}\\ \hline
        
        \text{\textbf{O} trained on random} & \makecell{\textbf{X} winrate: 15.73\% \\ drawrate: 23.24\%\\ \textbf{O} winrate: 61.08\%} & \makecell{\textbf{X} winrate: 43.31\% \\ drawrate: 30.02\%\\ \textbf{O} winrate: 26.67\%} \\ \hline
    \end{tabular}
    \caption{Matrix of results of agents trained on different opponents playing against each other.}
    \label{tab:matrix of agents playing against each other}
\end{table}



\section{Theoretical Q-Table compared to RL Q-Table}

The theoretical Q-Table generated using the minimax algorihtm has differences to the Q-Table generated through RL. \\\\

When generating the Q-Table of \textbf{X} playing against a perfect opponent, \textbf{O} will never end up in a loosing position. Hence states where \textbf{O} has a chance to loose are never explored and much of the RL generated Q-Table is unfilled. For the states which are explored the values of the RL generated Q-Table are similar to theoretical values as seen in Table \ref{tab:Q-Table comparison for ____o_oxx} \\\\

When generating the Q-Table of \textbf{X} playing against a random opponent, more states are explored, and two out of three actions which lead to a garanteed win (assuming perfect play), were given the q-value of 1 as seen in Table \ref{tab:Q-Table comparison for _______ox}. Although Likely, the reason why action (2, 2) was given the value of 0.00 is because it wasn't explored enough during the exploration phase.


%Q-Table example
\begin{table}[h]
\centering
\renewcommand{\arraystretch}{2.5}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
% Draws top line starting from 2nd column
\cline{1-10}
\multicolumn{1}{|c|}{\makecell{\_ \_ \_ \\ \_ o \_ \\ o x x}} & (1, 1) & (1, 2) & (1, 3) & (2, 1) & (2, 2) & (2, 3) & (3, 1) & (3, 2) & (3, 3) \\ \hline
\makecell{Theoretical Q-Table \\(perfect opponent)}& \cellcolor{black!60}& \cellcolor{black!60}  & \cellcolor{black!60}  & -1 & \cellcolor{black!60} & -1 & -1 & -1 & 0\\ \hline
RL Q-Table & \cellcolor{black!60}& \cellcolor{black!60} & \cellcolor{black!60} & -0.75 & \cellcolor{black!60} & -0.75 & -0.75 & -0.66 & 0\\ \hline
\end{tabular}
% 3. ADD THE CAPTION AND LABEL HERE
\caption{The theoretical and RL Q-Table for x playing against perfect opponent, give similar Q-Values for explored states}
\label{tab:Q-Table comparison for ____o_oxx} 
\end{table}

\begin{comment}
   array([-0.749944  , -0.65619606,  0.        , -0.74997125,  0.        ,
       -0.7499505 ,  0.        ,  0.        ,  0.        ])
       
    array([-1., -1.,  0., -1., nan, -1., nan, nan, nan])
\end{comment}
%Q-Table example
\begin{table}[h]
\centering
\renewcommand{\arraystretch}{2.5}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
% Draws top line starting from 2nd column
\cline{1-10}
\multicolumn{1}{|c|}{\makecell{\_ \_ \_ \\ \_ \_ \_ \\ \_ o x}} & (1, 1) & (1, 2) & (1, 3) & (2, 1) & (2, 2) & (2, 3) & (3, 1) & (3, 2) & (3, 3) \\ \hline
\makecell{Theoretical Q-Table \\(perfect opponent)}& 0 & \cellcolor{black!60} & \cellcolor{black!60} & 0 & 1 & 1  & 0 & 0 & 1\\ \hline
RL Q-Table & 0.062 & \cellcolor{black!60} & \cellcolor{black!60} & 0.00 & 1.00 & 0.00 & 0.06 & 0.02 & 0.83 \\ \hline
\end{tabular}
% 3. ADD THE CAPTION AND LABEL HERE
\caption{The theoretical Q-Table for x playing againsta  random opponent, and RL Q-Table for x playing against a random opponent give different values, because the future policy is different}
\label{tab:Q-Table comparison for _______ox} 
\end{table}

[ 0.  0.  1.  0.  1.  1.  0. nan nan]
[0.06251062 0.01562584 0.83249612 0.         0.99995493 0.
 0.06248338 0.         0.        ]

\chapter{Deep Q-Learning}
\textbf{Note:} AI was slightly used for the creating of the library, specifically it helped to create the function handling the backpropogation and the function handling the loss function\\

\noindent
\textbf{Note:} Although the library was mostly self made, in order to quickly test if it works (instead of spending a lot of time writing a game myself only to discover I have made an error with my neural network structure), I have made AI implement my library using 2 games made by Open-AI for training agents, ``cart pole'' and ``lunar lander''\\

The main disadvantage of traditional Q-Learning is that it only works well with a small number of discrete states. If the input is continuous, then it will no longer work, as the Q-Table would need to be infinitely large. This problem is solved by implementing a Deep Q-Network (DQN), instead of a Q-Table. While a Q-Table a state corresponds to all the Q-Values which can be taken by different actions, DQN estimates the Q-Values for all actions instead.\\

I have implemented a simple numpy-based library in python which allows for an implementation of a DQN.
\section{Neural Network structure, backpropogation, loss fucntion}

By structure DQN is identical to a normal neural network. It takes in the state such as coordinates or stock prices in the input layer and forward propogates the input through the network to return the estimated Q-Values in the output layer as seen in equation \ref{eq:forward propagation}. 

\begin{align}
    \mathbf{a}^{(l+1)} = \sigma \left( \mathbf{W}^{(l)} \mathbf{a}^{(l)} + \mathbf{b}^{(l)} \right)
    \label{eq:forward propagation}
\end{align}

The main difference between a Q-Table and a Q-Network is while during training a Q-Table, the Q-Values are simply updated using the Bellman equation, in a DQN weights and biases are adjusted. With a converged Q-Table $Q(s, a) = r + \gamma \max_{a'} Q(s', a')$ and very similarly, with a trained Deep Q-Network $Q(s, a, \theta) = r + \gamma \max_{a'} Q(s', a', \theta)$. During DQN training, an action picked is compared to the target as seen in equations \ref{eq:target} and \ref{eq:loss function}. The weights and biases $\theta$ are then adjusted through backpropogation with respect to the loss function $\mathcal{C}(\bm{\theta})$ as seen in listing \ref{lst:backprop}.

\begin{equation}
    \text{target} = r + \gamma \max_{a'} Q(s', a'; \bm{\theta}^-)
    \label{eq:target}
\end{equation}

\begin{equation}
    \mathcal{C}(\bm{\theta}) = \mathbb{E}_{(s, a, r, s')} \left[\frac{1}{2} \left( \text{target} - Q(s, a; \bm{\theta}) \right)^2 \right]
        \label{eq:loss function}
\end{equation}

\begin{lstlisting}[caption={Backpropagation implementation for Q-Learning}, label={lst:backprop}, language=Python]

dZ[actions, batch_indices] = predictions - targets #dC/dA

#backpropagate through the layers until we get to the input layer
for i in range(len(weights) -1, -1, -1):

    A_prev = forward_propagation_params_A[i]  #previous activation layer

    dW = 1/m * dZ @ A_prev.T #dC/dW = dC/dA * dA/dW
    db = 1/m * np.sum(dZ, axis=1, keepdims=True) #dC/db = dC/dA * dA/db

    dW_list.insert(0, dW)
    db_list.insert(0, db)


    if i > 0:
        #this is the dA/dZ = f'(Z) * dZ
        dZ = (weights[i].T @ dZ) * 
        functions_deriv[i - 1](forward_propagation_params_Z[i - 1])   
\end{lstlisting}

\section{Deep Q-Learning results}
Having made a simple library for implementing a DQN, I have prompted ``Gemini-3-flash-preview'' to implement it in the context of a game. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{"../../Part 2: Deep Q-Learning/fgiures/CartPole-v1_example.png"} % Path to your image
    \caption{The library I made is able to train a neural network to control a cart balancing a pole. The neural network takes in 4 parameters and generates 2 q-values.}
    \label{fig:CartPole-v1_example.png}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{"../../Part 2: Deep Q-Learning/fgiures/LunarLander-v3_example.png"} % Path to your image
    \caption{The library I made is able to train a neural network to control a space ship landing on the moon. The neural network takes in 8 parameters and generates 4 q-values.}
    \label{fig:LunarLander-v3_example.png}
\end{figure}

\chapter{Conclusion}

\chapter{Bibliography}
\end{document}
