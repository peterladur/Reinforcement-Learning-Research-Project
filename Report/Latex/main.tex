\documentclass{report}
\usepackage{amsmath, amssymb, amsthm} % Essential math tools

\title{Q-Learning}
\author{Peter (Petr) Ladur}
\date{}


\begin{document}
\maketitle

\chapter{Introduction}
The aim of this project is to explore Q-Learning in the context of tic-tac-toe and as other applicationsusing python. How variations in hyperparameters ($\alpha$, $\tau$, $r$) affect results was investigated to find optimal parameters for different scenarios. Deep-Q-Learning was explored in the context of a game and in the context of the stock market (haven't done yet, but will do once finish the report). \section{Q-Table}



\chapter{Background}

\section{Q-Learning}
Q-Learning is a form of reinforcement used in environment with discrete states. It relies on the Q-Table to store predicted expected values for each state $Q(\text{state}, \text{action})$. Initially the Q-Table can be initialised to random values, and after repeatidly exploring different states updating the predicted expected values for each state according to the ``Bellman equation''

\begin{align}
    Q_{new}(s_{t}, a) = (1-\alpha) \cdot Q(s_{t}, a) + \alpha (r + \gamma \cdot \text{max}{Q(s_{t + 1}, a)}) 
\end{align}

Where:\\
\begin{itemize}
    \item $Q(s, a)$ is the predicted outcome for the action at a particular state $s$
    \item $\alpha$ is the learning rate
    \item $\gamma$ is the discount factor
    \item $r$ is the immidiate reward for achiveing a state
\end{itemize}

\section{Tic-Tac-Toe}
Tic-Tac-Toe is a simple two-player game on a 3 by 3 grid. The game originated in ancient Egypt atleast 1300 BC. Players take turns placing X's and O's in the grid with the goal of getting 3 in a row. The game is drawn with perfect play from both sides, but O's have to be precise to garantee a draw. Using the ``minimax'' algorithm -assuming the oppponent will play the optimal move- a theoretical Q-Table can be derived.



\chapter{Q-Learning on tic-tac-toe}

\section{Algorithm}

\section{Optimal and non optimal opponent}

\section{Variations in $tau$ and $alpha$}
In general the Q-Learning governing equation has 4 hyperparameters ($\alpha$, $\tau$, $r$ and $\gamma$). The immidiate reward $r$ is by default set at $-1$ for loss, $0$ for draw, $1$ for  win and $0$ for just making a move. $\gamma$ is irrelevant as the future state matters as much as the current state (it doesn't matter if naughts loose on move 5 or on move 7, they still lost).\\

However, $\tau$ and $\alpha$ matter significantly for the learning rate. 
\chapter{Deep Q-Learning}

\chapter{Conclusion}

\chapter{Bibliography}
\end{document}
