\documentclass{report}
\usepackage{amsmath, amssymb, amsthm} % Essential math tools
\usepackage[table]{xcolor}
\usepackage[hidelinks]{hyperref} % hidelinks removes the ugly red/green boxes around links
\usepackage{cleveref}   
\usepackage[utf8]{inputenc}
\usepackage{makecell} % Required for multi-line cells
\usepackage{comment}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{float}

\usepackage{bm}

\lstset{
    basicstyle=\ttfamily\small,        % Use typewriter font, small size
    breaklines=true,                   % !!! This fixes code running off the page
    breakatwhitespace=true,            % Only break lines at spaces
    columns=fullflexible,              % !!! This fixes the weird character spacing
    keepspaces=true,                   % Keeps indentation spaces
    numbers=left,                      % Line numbers on the left
    numberstyle=\tiny\color{gray},     % Style of line numbers
    frame=single,                      % Adds a box around the code
    rulecolor=\color{black},           % Color of the box
    commentstyle=\color{green!50!black}, % Color of comments
    keywordstyle=\color{blue},         % Color of keywords
    showstringspaces=false,            % Don't show dots for spaces
    xleftmargin=15pt,                  % Indent the whole block slightly
}

% This defines a "Pseudocode" style for listings
\lstdefinelanguage{Pseudocode}{
    morekeywords={for, in, if, else, for each, while, return, end},
    sensitive=false,
    morecomment=[l]{//},
    morestring=[b]",
}

\title{\textbf{Q-Learning}}
\author{Peter (Petr) Ladur\\\\56445760\\\\Supervised by: xxx yyy}
\date{}

\begin{document}
\maketitle
\chapter*{Acknowledgements}
I would like to express my gratitude to my supervisor xxx yyy for their guidance and technical feedback throughout this summer research project. \\\\

\noindent
I would also like to thank the Mathematics Department at [my uni] for allowing me to undertake this summer research project.

\chapter{Introduction}
The aim of this project is to explore Q-Learning in the context of tic-tac-toe and other applications using Python. Theoretical Q-Table was generated using the ``minimax'' algorithm, and was compared to Q-Tables generated through training against both, random and perfect opponent. Results of agents trained on different policies playing against each other were recorded. How hyperparameters ($\alpha$, $\tau$) affect training results was investigated and an optimal function for hyperparameters was proposed. Deep-Q-Learning was investigated in the context of different games in the ``Open-AI gymnasium''.

\chapter{Background}
\section{Q-Table}
Q-Table is a table which contains the total expected reward after every possible action in a particular state assuming the optimal policy is followed. In the context of tic-tac-toe the state is the game position at that particular moment, and actions are all the empty squares. As seen in \cref{tab:perfect_Q_Table_entry}, for that particular state there are four already occupied positions, two actions that lead to a total expected reward of $1$ (wins) and three actions which lead to a total expected reward of $0$ (draws) assuming perfect play. The Q-Table can then be used to select moves which maximise the total reward, creating a perfect tic-tac-toe bot. 

%Q-Table example
\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
% Draws top line starting from 2nd column
\cline{2-10}
\multicolumn{1}{c|}{} & (1, 1) & (1, 2) & (1, 3) & (2, 1) & (2, 2) & (2, 3) & (3, 1) & (3, 2) & (3, 3) \\ \hline
\makecell{\_ \_ o \\ \_ x \_ \\ x o \_}  & 
% This is the "No-Package" multi-line trick:
\cellcolor{black!60}& 
1 & 1 & \cellcolor{black!60} & \cellcolor{black!60} & 0 & 0 & 0 & \cellcolor{black!60}\\ \hline
\end{tabular}
% 3. ADD THE CAPTION AND LABEL HERE
\caption{Two actions lead to a win, three actions lead to a draw assuming perfect play from both sides}
\label{tab:perfect_Q_Table_entry} 
\end{table}
 
\section{Q-Learning}
In the context of Q-Learning, the Q-Table can be filled up by playing games and updating the values of actions taken during that game according to the Bellman equation. 

\begin{align}
Q(s, a) \leftarrow (1-\alpha) \cdot Q(s, a) + \alpha (r + \gamma \cdot \max_{a'}{Q(s', a')}) 
\end{align}

Where:
\begin{itemize}
    \item $Q(s, a)$ is the predicted outcome for the action at a particular state $s$
    \item $s'$ and $a'$ are the future state and action respectively
    \item $0 \leq \alpha \leq 1$  is the learning rate
    \item $0 \leq \gamma \leq 1$ is the discount factor
    \item $r$ is the immediate reward for achieving a state
\end{itemize}

The learning rate $\alpha$ determines how much does the new information replace the already known information. A learning rate of $1$ would mean that each training iteration the $Q(s, a)$ is completely replaced by $(r + \gamma \cdot \max_{a}{Q(s_{t + 1}, a)})$. A learning rate of $0$ would imply that $Q(s, a)$ is not affected by training at all. The learning rate can be adjusted during training as the model becomes more stable.\\

The immediate reward in the context of tic-tac-toe is $0$ if the game is not terminated. If the game is terminated, the immediate reward $r$ is $1$ for a game that is won by \textbf{X}, $0$ for a game that is drawn and $-1$ for a game that is won by \textbf{O}. \\

The discount factor is a measure of how much the future is important compared to the current decision. For example in the context of tic-tac-toe the future matters just as much as the current situation, no matter if you lose on move 5 or move 7, you still lose. Meanwhile, in a long game with no set win condition, such as a player trying to dodge obstacles, often the most important thing is not to die right now, the future is less important.


\chapter{Q-Learning on tic-tac-toe}

\section{Generating theoretical Q-Table}
The ``minimax'' algorithm can be used in turn-based games to calculate the best policy. It is designed to minimise the potential loss and maximise the potential gain. The algorithm works by first generating all the possible final states and then taking turns ``undoing'' the moves that could have lead to that state by removing \textbf{X}'s and \textbf{O}'s until the initial state (empty board) is reached. Each time an \textbf{X} or \textbf{O} is removed the worst case scenario is considered, filling up the theoretical Q-Table as seen in code box \ref{lst:minimax}. This explores the game trees and backproagates the values, filling up a theoretical Q-Table.

\begin{lstlisting}[caption={Minimax algorithm for generating the theoretical Q-Table}, label={lst:minimax}, language=Pseudocode]
terminal_states = generate_all_terminal_states()

for parent_state in parent_states:
    
    for x/o in parent_state:

        //"undoing" a move from a state
        child_state = parent_state with an x/o removed

        if child_state is terminal_state:
            Q_Table[child_state] = [game_result] * 9
        else:
            //min/max depends on whether x/o removed
            Q_Table[child_state][index of x/o] = min/max(Q_Table[parent_state])  

            parent_states = child states
\end{lstlisting}



\section{Q-Learning implementation}

During the Q-Learning algorithm batches of games were played. The agent would choose moves using a weighted probability values $p$ according to a slightly modified Boltzmann function as seen in \eqref{modified Boltzmann}. States $s$, actions taken $a$ and results $r$ are stored in a queue. The queue is then emptied and Q-Table is updated using the Bellman equation as seen in \eqref{terminal state update} and \eqref{non terminal state update}.\\\\

\begin{align}
    p_{i}(\tau) = \frac{\tau^{\epsilon_{i}}}{\displaystyle \sum_{j=1}^{9} \tau^{\epsilon_{j}}}
\label{modified Boltzmann}
\end{align}

Where $\epsilon_{i} = Q(s, a_{i})$\\

\textbf{If terminal state:}
\begin{align}
    Q(s_{t}, a) \leftarrow (1-\alpha) \cdot Q(s_{t}, a) + \alpha r
\label{terminal state update}
\end{align}

\textbf{If non-terminal state: }
\begin{align}
    Q(s_{t}, a) \leftarrow (1-\alpha) \cdot Q(s_{t}, a) + \alpha \max_{a}{Q(s_{t + 1}, a)} 
\label{non terminal state update}
\end{align}

As agents learn their draw rate or win rate depending on if they are playing against a perfect or random opponent increases in the form of the decaying exponential. As \textbf{X} learns against optimal \textbf{O} the draws increase in the form of a decaying exponential draws \% = $e^{-k \cdot \text{games played}}$ as seen in \cref{fig:x_vs_perfect_distribution_1}. While \textbf{X} learns against a random \textbf{O} the proportion of \textbf{X} wins increases exponentially as seen in \cref{fig:x_vs_random_distribution_1}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{"../../Part 1: Tic Tac Toe/figures/x_vs_perfect_distribution_1.png"} % Path to your image
    \caption{\textbf{X} training against perfect \textbf{O} causes the draw rate to increase in the form of a decaying exponential}
    \label{fig:x_vs_perfect_distribution_1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{"../../Part 1: Tic Tac Toe/figures/x_vs_random_distribution_1.png"} % Path to your image
    \caption{\textbf{X} training against random \textbf{O} causes the \textbf{X} win rate to increase in the form of a decaying exponential}
    \label{fig:x_vs_random_distribution_1}
\end{figure}

\section{Variations in $\alpha$ and $\tau$}
The values of hyperparameters $\tau$ and $\alpha$  matter significantly. During the game it is best to decrease the learning rate. Initially it desired for the new information to rapidly change the Q-Table, but eventually a low learning rate means that one bad game will not significantly affect the Q-Values trained over hundreds of games in the past. Meanwhile, $\tau$ should increase switching from exploration to exploitation. Different models for $\tau$ and $\alpha$ have been tested, and two models are proposed as best candidates as seen in 

\begin{align}
    \alpha(\text{decay rate}) = \alpha_{initial} \cdot e^{-\text{decay rate} \times \text{game number}}\\
    \tau(\text{growth rate}) = \text{Maximum Rate}^{((\frac{\text{game number}}{\text{total games}})^{\text{growth rate}})}
\end{align}

In order to pick out the best parameters $a$ and $t$ for $\alpha$ and $\tau$ respectively, a range of $a$ and $t$ was picked, and an agent was trained for each combination as seen in \cref{x_vs_perfect_range_of_params_1.png}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{"../../Part 1: Tic Tac Toe/figures/x_vs_perfect_range_of_params_1.png"} % Path to your image
    \caption{After training \textbf{X} on $1000$ batches of $10$ games, the highest performing values for parameters \textit{growth rate} are $10^{\frac{-1}{3}}$ and \textit{decay rate} between $10^{-4}$ and $10^{-10}$}
    \label{x_vs_perfect_range_of_params_1.png}
\end{figure}

\begin{align*}
    \alpha(\text{decay rate}) = \alpha_{initial} \cdot e^{-10^{-7} \times \text{game number}}\\
    \tau(\text{growth rate}) = \text{1000}^{((\frac{\text{game number}}{\text{total games}})^{10^{\frac{-1}{3}}})}
\end{align*}

\section{Optimal and non-optimal opponent}

After training different agents, I have made them play 10000 against each other collecting results in \cref{tab:matrix of agents playing against each other}. 

\begin{itemize}
    \item An agent trained against the random policy does very well against an agent trained on an optimal policy. The reason for this, is that an agent trained on the perfect policy can only choose the moves at random if its opponent makes a mistake. For example \textbf{X} trained on perfect would have never been exposed to a state where it can win, so it will not know where to go and will make a random move.
    \item \textbf{X} and \textbf{O} trained on a perfect opponent perform very close to a random performance in a match against each other. The likely reason for this, is that both of the opponents have been trained to draw and have not encountered the states where a non-optimal move is played.
    \item The likely reason for high \textbf{X} and \textbf{O} wins for agents trained on a random policy, is that they expect their opponent to continue playing the game randomly. Sometimes they might go for ``risky'' plays which would have a high probability of a win/draw against a random opponent, but encounter resistance against an actually trained opponent.
\end{itemize}

\begin{table}[H]
    \centering
    \renewcommand{\arraystretch}{3} % Extra height for your RL board states
    \begin{tabular}{|l|c|c|}
        % 1. Draw the top horizontal line ONLY for columns 2 and 3
        \cline{1-3}
        
        % 2. Use multicolumn to remove the left-most border of the first cell
        \textbf{Agent} & \text{\textbf{X} trained on perfect} & \text{\textbf{X} trained on random} \\ \hline
        
        \text{\textbf{O} trained on perfect} & \makecell{\textbf{X} win rate: 58.89\% \\ draw rate: 14.12\%\\ \textbf{O} win rate:  26.99\%} & \makecell{\textbf{X} win rate: 86.05\% \\ draw rate: 8.36\%\\ \textbf{O} win rate: 5.59\%}\\ \hline
        
        \text{\textbf{O} trained on random} & \makecell{\textbf{X} win rate: 15.73\% \\ draw rate: 23.24\%\\ \textbf{O} win rate: 61.08\%} & \makecell{\textbf{X} win rate: 43.31\% \\ draw rate: 30.02\%\\ \textbf{O} win rate: 26.67\%} \\ \hline
    \end{tabular}
    \caption{Matrix of results of agents trained on different opponents playing against each other.}
    \label{tab:matrix of agents playing against each other}
\end{table}



\section{Theoretical Q-Table compared to RL-estimated Q-Table}

The theoretical Q-Table generated using the minimax algorithm has differences to the Q-Table generated through RL. \\\\

When generating the Q-Table of \textbf{X} playing against a perfect opponent, \textbf{O} will never end up in a losing position. Hence, states where \textbf{O} has a chance to lose are never explored and much of the RL generated Q-Table is unfilled. For the states which are explored the values of the RL generated Q-Table are similar to theoretical values as seen in \cref{tab:Q-Table comparison for ____o_oxx} \\\\

When generating the Q-Table of \textbf{X} playing against a random opponent, more states are explored, and two out of three actions which lead to a guaranteed win (assuming perfect play), were given the q-value of 1 as seen in \cref{tab:Q-Table comparison for _______ox}. Likely, the reason why action (2, 3) was given the value of 0.00 is because it wasn't explored enough during the exploration phase.


%Q-Table example
\begin{table}[H]
\centering
\renewcommand{\arraystretch}{2.5}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
% Draws top line starting from 2nd column
\cline{1-10}
\multicolumn{1}{|c|}{\makecell{\_ \_ \_ \\ \_ o \_ \\ o x x}} & (1, 1) & (1, 2) & (1, 3) & (2, 1) & (2, 2) & (2, 3) & (3, 1) & (3, 2) & (3, 3) \\ \hline
\makecell{Theoretical Q-Table \\(perfect opponent)}& \cellcolor{black!60}& \cellcolor{black!60}  & \cellcolor{black!60}  & -1 & \cellcolor{black!60} & -1 & -1 & -1 & 0\\ \hline
RL-estimated Q-Table & \cellcolor{black!60}& \cellcolor{black!60} & \cellcolor{black!60} & -0.75 & \cellcolor{black!60} & -0.75 & -0.75 & -0.66 & 0\\ \hline
\end{tabular}
% 3. ADD THE CAPTION AND LABEL HERE
\caption{The theoretical and RL-estimated Q-Table for x playing against perfect opponent, give similar Q-Values for explored states}
\label{tab:Q-Table comparison for ____o_oxx} 
\end{table}

%Q-Table example
\begin{table}[H]
\centering
\renewcommand{\arraystretch}{2.5}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
% Draws top line starting from 2nd column
\cline{1-10}
\multicolumn{1}{|c|}{\makecell{\_ \_ \_ \\ \_ \_ \_ \\ \_ o x}} & (1, 1) & (1, 2) & (1, 3) & (2, 1) & (2, 2) & (2, 3) & (3, 1) & (3, 2) & (3, 3) \\ \hline
\makecell{Theoretical Q-Table \\(perfect opponent)}& 0 & \cellcolor{black!60} & \cellcolor{black!60} & 0 & 1 & 1  & 0 & 0 & 1\\ \hline
RL-estimated Q-Table & 0.062 & \cellcolor{black!60} & \cellcolor{black!60} & 0.00 & 1.00 & 0.00 & 0.06 & 0.02 & 0.83 \\ \hline
\end{tabular}
% 3. ADD THE CAPTION AND LABEL HERE
\caption{The theoretical Q-Table for x playing against a random opponent, and RL-estimated Q-Table for x playing against a random opponent give different values, because the future policy is different}
\label{tab:Q-Table comparison for _______ox} 
\end{table}



\chapter{Deep Q-Learning}
The main disadvantage of traditional Q-Learning is that it only works well with a few discrete states. If the input is continuous, then it will no longer work, as the Q-Table would need to be infinitely large. This problem is solved by implementing a Deep Q-Network (DQN), instead of a Q-Table. While in a Q-Table a state corresponds to all the exact Q-Values which can be taken by different actions, DQN estimates the Q-Values for all actions instead.\\

I have implemented a simple NumPy-based library in Python which allows for an implementation of a DQN.
\section{Neural Network structure, backpropagation, cost function}

By structure DQN is identical to a normal neural network. It takes in the state such as coordinates or stock prices in the input layer and forward propagates the input through the network to return the estimated Q-Values for different actions in the output layer as seen in equation \eqref{eq:forward propagation}. 

\begin{align}
    \mathbf{a}^{(l+1)} = \sigma \left( \mathbf{W}^{(l)} \mathbf{a}^{(l)} + \mathbf{b}^{(l)} \right)
    \label{eq:forward propagation}
\end{align}

The main difference between a Q-Table and a Q-Network is while during training a Q-Table, the Q-Values are simply updated using the Bellman equation, in a DQN weights and biases are adjusted. With a converged Q-Table $Q(s, a) = r + \gamma \max_{a'} Q(s', a')$ and very similarly, with a trained Deep Q-Network $Q(s, a, \theta) = r + \gamma \max_{a'} Q(s', a', \theta)$. During DQN training, an action picked is compared to the target as seen in equations \eqref{eq:target} and \eqref{eq:loss function}. The weights and biases $\theta$ are then adjusted through backpropagation with respect to the cost function $\mathcal{C}(\bm{\theta})$ as seen in code box \ref{lst:backprop}. When calculating the target, a copy of the DQN with constant parameters $\theta^{-}$ is used which are updated to match the parameters $\theta$ every N steps. 

\begin{equation}
    \text{target} = r + \gamma \max_{a'} Q(s', a'; \bm{\theta}^-)
    \label{eq:target}
\end{equation}

\begin{equation}
    \mathcal{C}(\bm{\theta}) = \mathbb{E}_{(s, a, r, s')} \left[\frac{1}{2} \left ( Q(s, a; \bm{\theta}) - \text{target}) \right)^2 \right]
        \label{eq:loss function}
\end{equation}

\begin{lstlisting}[caption={Backpropagation implementation for Q-Learning}, label={lst:backprop}, language=Python]

dZ[actions, batch_indices] = predictions - targets #dC/dA

#backpropagate through the layers until we get to the input layer
for i in range(len(weights) -1, -1, -1):

    A_prev = forward_propagation_params_A[i]  #previous activation layer

    dW = 1/m * dZ @ A_prev.T #dC/dW = dC/dA * dA/dW
    db = 1/m * np.sum(dZ, axis=1, keepdims=True) #dC/db = dC/dA * dA/db

    dW_list.insert(0, dW)
    db_list.insert(0, db)


    if i > 0:
        #this is the dA/dZ = f'(Z) * dZ
        dZ = (weights[i].T @ dZ) * 
        functions_deriv[i - 1](forward_propagation_params_Z[i - 1])   
\end{lstlisting}

\section{Deep Q-Learning results}
Having made a simple library for implementing a DQN, I have prompted ``Gemini-3-flash-preview'' to implement it in the context of a game. 


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{"../../Part 2: Deep Q-Learning/figures/CartPole-v1_example.png"} % Path to your image
    \caption{The library I made is able to train a neural network to control a cart balancing a pole. The neural network takes in 4 parameters and generates 2 q-values.}
    \label{fig:CartPole-v1_example.png}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{"../../Part 2: Deep Q-Learning/figures/LunarLander-v3_example.png"} % Path to your image
    \caption{The library I made is able to train a neural network to control a spaceship landing on the moon. The neural network takes in 8 parameters and generates 4 q-values.}
    \label{fig:LunarLander-v3_example.png}
\end{figure}
\chapter{Conclusion}
In this project I have succeeded in implementing traditional Q-Learning in the context of tic-tac-toe and with help of AI implemented Deep Q-Learning in the context of two simple games. During Q-Learning in tic-tac-toe agents learn in an exponential manner against perfect and a random opponent drawing and winning more respectively. The most notable feature discovered when making agents trained on different policies play against each other, was that agents trained on the random policy would outperform agents trained on the perfect policy. The RL-estimated Q-Table was compared to a theoretical Q-Table was similar during training against a perfect opponent but was different against a random opponent due to differences in future policy. During the Deep Q-Learning implementation, a small NumPy-based library was developed, which could then be applied to various environments, such as the ``Cart Pole'' and ``Lunar Lander'' games developed by Open-AI.

\chapter{Future Work}

During this project there were several areas which I would like to explore further.\\

\begin{itemize}
    \item Experiment with other hyperparameter functions such as piecewise functions. Particularly, to only explore initially, and only then start exploring.
    \item Experiment with the discount factor $\gamma$. Although it doesn't matter if you win on move $5$ or move $7$, winning faster is preferred, as it is still possible for an agent to make a mistake in the future
    \item Implementing a DQN in the context of a stock market. I would like to input historical data into a DQN and train it to sell, buy or hold, trying to make profit
\end{itemize}


\chapter{Appendices}

\section{Declaration}

\textbf{Note:} AI was slightly used for the creating of the library, specifically it helped to create the function handling the backpropagation and the function handling the loss function\\

\noindent
\textbf{Note:} Although the library was mostly self-made, in order to quickly test if it works (instead of spending a lot of time writing a game myself only to discover I have made an error with my neural network structure), I have made AI implement my library using 2 games made by Open-AI for training agents, ``cart pole'' and ``lunar lander''\\

\section{Code}

All the code for this project along with the report is accessible in a public GitHub repository. It can be accessed via this link:\\ \url{https://github.com/peterladur/Reinforcement-Learning-Research-Project}

\section{References}



\end{document}
