\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{2}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Q-Table}{2}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Two actions lead to a win, three actions lead to a draw assuming perfect play from both sides}}{2}{}\protected@file@percent }
\newlabel{tab:perfect_Q_Table_entry}{{2.1}{2}{}{table.2.1}{}}
\newlabel{tab:perfect_Q_Table_entry@cref}{{[table][1][2]2.1}{[1][2][]2}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Q-Learning}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Q-Learning on tic-tac-toe}{4}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Generating theoretical Q-Table}{4}{}\protected@file@percent }
\newlabel{lst:minimax}{{3.1}{4}{}{lstlisting.3.1}{}}
\newlabel{lst:minimax@cref}{{[lstlisting][1][3]3.1}{[1][4][]4}{}{}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.1}Minimax algorithm for generating the theoretical Q-Table}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Q-Learning implementation}{5}{}\protected@file@percent }
\newlabel{modified bolztman}{{3.1}{5}{}{equation.3.1}{}}
\newlabel{modified bolztman@cref}{{[equation][1][3]3.1}{[1][5][]5}{}{}{}}
\newlabel{terminal state update}{{3.2}{5}{}{equation.3.2}{}}
\newlabel{terminal state update@cref}{{[equation][2][3]3.2}{[1][5][]5}{}{}{}}
\newlabel{non terminal state update}{{3.3}{5}{}{equation.3.3}{}}
\newlabel{non terminal state update@cref}{{[equation][3][3]3.3}{[1][5][]5}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Variations in $\alpha $ and $\tau $}{5}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces \textbf  {X} training against perfect \textbf  {O} causes the drawrate in the form of a decaying exponential}}{6}{}\protected@file@percent }
\newlabel{fig:x_vs_perfect_distribution_1}{{3.1}{6}{}{figure.3.1}{}}
\newlabel{fig:x_vs_perfect_distribution_1@cref}{{[figure][1][3]3.1}{[1][5][]6}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Optimal and non optimal opponent}{6}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces \textbf  {X} training against perfect \textbf  {O} causes the drawrate in the form of a decaying exponential}}{7}{}\protected@file@percent }
\newlabel{fig:x_vs_random_distribution_1}{{3.2}{7}{}{figure.3.2}{}}
\newlabel{fig:x_vs_random_distribution_1@cref}{{[figure][2][3]3.2}{[1][5][]7}{}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Matrix of results of agents trained on different opponents playing against each other.}}{7}{}\protected@file@percent }
\newlabel{tab:matrix of agents playing against each other}{{3.1}{7}{}{table.3.1}{}}
\newlabel{tab:matrix of agents playing against each other@cref}{{[table][1][3]3.1}{[1][6][]7}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Theoretical Q-Table compared to RL-estimated Q-Table}{7}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces After training \textbf  {X} on $1000$ batches of $10$ games, the highest performing values for parameters \textit  {growth rate} are $10^{\frac  {-1}{3}}$ and \textit  {decay rate} between $10^{-4}$ and $10^{-10}$ respectively}}{8}{}\protected@file@percent }
\newlabel{x_vs_perfect_range_of_params_1.png}{{3.3}{8}{}{figure.3.3}{}}
\newlabel{x_vs_perfect_range_of_params_1.png@cref}{{[figure][3][3]3.3}{[1][5][]8}{}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces The theoretical and RL-estimated Q-Table for x playing against perfect opponent, give similar Q-Values for explored states}}{9}{}\protected@file@percent }
\newlabel{tab:Q-Table comparison for ____o_oxx}{{3.2}{9}{}{table.3.2}{}}
\newlabel{tab:Q-Table comparison for ____o_oxx@cref}{{[table][2][3]3.2}{[1][8][]9}{}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces The theoretical Q-Table for x playing againsta random opponent, and RL-estimated Q-Table for x playing against a random opponent give different values, because the future policy is different}}{9}{}\protected@file@percent }
\newlabel{tab:Q-Table comparison for _______ox}{{3.3}{9}{}{table.3.3}{}}
\newlabel{tab:Q-Table comparison for _______ox@cref}{{[table][3][3]3.3}{[1][8][]9}{}{}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Deep Q-Learning}{10}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Neural Network structure, backpropagation, cost fucntion}{10}{}\protected@file@percent }
\newlabel{eq:forward propagation}{{4.1}{10}{}{equation.4.1}{}}
\newlabel{eq:forward propagation@cref}{{[equation][1][4]4.1}{[1][10][]10}{}{}{}}
\newlabel{eq:target}{{4.2}{11}{}{equation.4.2}{}}
\newlabel{eq:target@cref}{{[equation][2][4]4.2}{[1][10][]11}{}{}{}}
\newlabel{eq:loss function}{{4.3}{11}{}{equation.4.3}{}}
\newlabel{eq:loss function@cref}{{[equation][3][4]4.3}{[1][11][]11}{}{}{}}
\newlabel{lst:backprop}{{4.1}{11}{}{lstlisting.4.1}{}}
\newlabel{lst:backprop@cref}{{[lstlisting][1][4]4.1}{[1][11][]11}{}{}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.1}Backpropagation implementation for Q-Learning}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Deep Q-Learning results}{11}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces The library I made is able to train a neural network to control a cart balancing a pole. The neural network takes in 4 parameters and generates 2 q-values.}}{12}{}\protected@file@percent }
\newlabel{fig:CartPole-v1_example.png}{{4.1}{12}{}{figure.4.1}{}}
\newlabel{fig:CartPole-v1_example.png@cref}{{[figure][1][4]4.1}{[1][11][]12}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces The library I made is able to train a neural network to control a space ship landing on the moon. The neural network takes in 8 parameters and generates 4 q-values.}}{12}{}\protected@file@percent }
\newlabel{fig:LunarLander-v3_example.png}{{4.2}{12}{}{figure.4.2}{}}
\newlabel{fig:LunarLander-v3_example.png@cref}{{[figure][2][4]4.2}{[1][11][]12}{}{}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion}{13}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Appendices}{14}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Declaration}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}References}{14}{}\protected@file@percent }
\gdef \@abspage@last{15}
