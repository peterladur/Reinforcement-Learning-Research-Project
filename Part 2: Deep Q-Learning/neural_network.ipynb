{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c204aee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67237b4a",
   "metadata": {},
   "source": [
    "For deep q learning, there are a couple of differences in back prop we have to set up, but overall it's very similar to a typical NN setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc6564",
   "metadata": {},
   "source": [
    "Helper Functions Set Up:\n",
    "\n",
    "    - ReLU\n",
    "\n",
    "    - softmax\n",
    "\n",
    "    - derivative of ReLU\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c94c27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(Z):\n",
    "    \"\"\"Applies ReLU\"\"\"\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"Applies softmax\"\"\"\n",
    "    return np.exp(Z)/np.sum(np.exp(Z))\n",
    "\n",
    "def deriv_ReLU(Z):\n",
    "    \"\"\"Diffirentiates ReLU\"\"\"\n",
    "    return Z > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0b2ace",
   "metadata": {},
   "source": [
    "NN functionality:\n",
    "\n",
    "    - init params\n",
    "\n",
    "    - forward prop\n",
    "\n",
    "    - back prop\n",
    "\n",
    "    - updating params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(nn_structure:list):\n",
    "    \"\"\"This function sets up initial parameters W1, b1, W2, b2, ... following the given structure\"\"\"\n",
    "\n",
    "    #weights and biases will contain np matrices of the weights and biases for each layer such that\n",
    "    #weight[n] = n'th layer weights, etc\n",
    "    weights = []\n",
    "    biases = []\n",
    "    \n",
    "    for layer_size_index in range(1, len(nn_structure)):\n",
    "        \n",
    "        #current layer starting at 1st\n",
    "        current_layer_size = nn_structure[layer_size_index]\n",
    "        #previous layer start at 0th (input)\n",
    "        previous_layer_size = nn_structure[layer_size_index - 1]\n",
    "\n",
    "        W = np.random.rand(current_layer_size, previous_layer_size) - 0.5\n",
    "        b = np.random.rand(current_layer_size, 1) - 0.5\n",
    "\n",
    "        weights.append(W)\n",
    "        biases.append(b)\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "def forward_propogate(weights, biases ,input_layer, functions):\n",
    "    \"\"\"forward propogates the NN using inserted params & functions, given the input layer\"\"\"\n",
    "\n",
    "    s = input_layer.copy() #just for easier notation\n",
    "    nn_length = len(weights)\n",
    "\n",
    "    #will be eventually returned\n",
    "    forward_propogation_params_A = [s]\n",
    "    forward_propogation_params_Z = []\n",
    "    \n",
    "    #forward propogates\n",
    "    for i in range(nn_length):\n",
    "        #Note that A_0 = s\n",
    "        #Z_n = W_n @ A_(n-1) + b_n\n",
    "        Z = weights[i] @ forward_propogation_params_A[-1] + biases[i]\n",
    "        \n",
    "        #A_n = activation function(Z_n)\n",
    "        A = functions[i](Z)\n",
    "        print(A)\n",
    "\n",
    "        forward_propogation_params_Z.append(Z)\n",
    "        forward_propogation_params_A.append(A)\n",
    "\n",
    "    \n",
    "    \n",
    "    return forward_propogation_params_A, forward_propogation_params_Z\n",
    "\n",
    "\n",
    "def back_propogate(forward_propogation_params_A, forward_propogation_params_Z, weights, actions, targets, functions_deriv):\n",
    "    \"\"\"\n",
    "    weights: list of weight matrices\n",
    "    forward_propogation_params_A: list of all A matrices from forward_propagation\n",
    "    forward_propogation_params_Z: list of all Z matrices from forward_propagation\n",
    "    actions: array of action indices taken (batch_size,)\n",
    "    targets: array of target y-values (batch_size,)\n",
    "    functions_deriv: list of derivative functions for each layer\n",
    "    \"\"\"\n",
    "    #note this is written with a decent amount of help from AI\n",
    "    #I udnerstand most of this tho, but some of the matrix calculus is confusing\n",
    "    #AI is unmatched sometimes, it would take a human a long long long time to figure this out\n",
    "\n",
    "\n",
    "    m = actions.shape[0] #size of the batch\n",
    "\n",
    "    #gradient storate will later be returned\n",
    "    dW_list = []\n",
    "    db_list = []\n",
    "\n",
    "    #grab output layer\n",
    "    final_A = forward_propogation_params_A[-1]\n",
    "\n",
    "    #set everything else to 0, except for the decision which was made\n",
    "    dZ = np.zeros_like(final_A)\n",
    "    batch_indices = np.arange(m)\n",
    "    predictions = final_A[actions, batch_indices]\n",
    "    dZ[actions, batch_indices] = predictions - targets\n",
    "\n",
    "    #backpropogate through the layers until we get to the input layer\n",
    "    for i in range(len(weights) -1, 0, -1):\n",
    "        #a bunch of matrix calculus I am semi familiar with, but want to understand better\n",
    "        A_prev = forward_propogation_params_A[i] \n",
    "\n",
    "        dW = 1/m * dZ @ A_prev.T\n",
    "        db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "        dW_list.insert(0, dW)\n",
    "        db_list.insert(0, db)\n",
    "\n",
    "\n",
    "        if i > 0:\n",
    "            dZ = (weights[i].T @ dZ) * functions_deriv[i - 1](forward_propogation_params_Z[i - 1])\n",
    "\n",
    "    return dW_list, db_list\n",
    "\n",
    "def update_params(weights, biases, back_propogration_weights, back_propogration_biases, alpha):\n",
    "    \"\"\"updates all the params in the neural netwrok\"\"\"\n",
    "\n",
    "    nn_length = len(weights)\n",
    "\n",
    "    \n",
    "    #update everything\n",
    "    for i in range(nn_length):\n",
    "        #split for notation\n",
    "        W = weights[i]\n",
    "        b = biases[i]\n",
    "\n",
    "        dW = back_propogration_weights[i]\n",
    "        db = back_propogration_biases[i]\n",
    "\n",
    "\n",
    "        #update the biases\n",
    "        W -= alpha * dW\n",
    "        b -= alpha * db\n",
    "\n",
    "    return weights, biases #Since arrays are pointers, I can just return the orignial list of arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e11506c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [3]]\n",
      "[[0.        ]\n",
      " [0.27203627]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]]\n",
      "[[0.58401455]\n",
      " [0.41598545]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([[0.        ],\n",
       "         [0.27203627],\n",
       "         [0.        ],\n",
       "         [0.        ],\n",
       "         [0.        ]]),\n",
       "  array([[0.58401455],\n",
       "         [0.41598545]])],\n",
       " [array([[-0.47513828],\n",
       "         [ 0.27203627],\n",
       "         [-0.78737495],\n",
       "         [-1.32470772],\n",
       "         [-0.50509163]]),\n",
       "  array([[-0.08670247],\n",
       "         [-0.42597809]])])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_structue = [3, 5, 2]\n",
    "\n",
    "functions = [ReLU, softmax]\n",
    "\n",
    "weights, biases = init_params(nn_structue)\n",
    "\n",
    "X = np.array([[1, 2, 3]]).T\n",
    "print(X)\n",
    "forward_propogate(weights, biases,X, functions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1041c437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(10 -1, -1, -1):\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
