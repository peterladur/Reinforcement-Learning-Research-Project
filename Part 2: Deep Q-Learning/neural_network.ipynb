{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c204aee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67237b4a",
   "metadata": {},
   "source": [
    "For deep q learning, there are a couple of differences in back prop we have to set up, but overall it's very similar to a typical NN setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc6564",
   "metadata": {},
   "source": [
    "Helper Functions Set Up:\n",
    "\n",
    "    - ReLU\n",
    "\n",
    "    - softmax\n",
    "\n",
    "    - derivative of ReLU\n",
    "\n",
    "    - identity\n",
    "    \n",
    "    - derivative of identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0c94c27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(Z):\n",
    "    \"\"\"Applies ReLU\"\"\"\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"Applies softmax\"\"\"\n",
    "    return np.exp(Z)/np.sum(np.exp(Z))\n",
    "\n",
    "def deriv_ReLU(Z):\n",
    "    \"\"\"Diffirentiates ReLU\"\"\"\n",
    "    return Z > 0\n",
    "\n",
    "def identity(Z):\n",
    "    \"\"\"Does nothing\"\"\"\n",
    "    return Z\n",
    "\n",
    "def deriv_identity(Z):\n",
    "    \"\"\"Returns 1 like Z\"\"\"\n",
    "    return np.ones_like(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0b2ace",
   "metadata": {},
   "source": [
    "NN functionality:\n",
    "\n",
    "    - init params\n",
    "\n",
    "    - forward prop\n",
    "\n",
    "    - back prop\n",
    "\n",
    "    - updating params\n",
    "\n",
    "    - training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(nn_structure:list):\n",
    "    \"\"\"This function sets up initial parameters W1, b1, W2, b2, ... following the given structure\"\"\"\n",
    "\n",
    "    #weights and biases will contain np matrices of the weights and biases for each layer such that\n",
    "    #weight[n] = n'th layer weights, etc\n",
    "    weights = []\n",
    "    biases = []\n",
    "    \n",
    "    for layer_size_index in range(1, len(nn_structure)):\n",
    "        \n",
    "        #current layer starting at 1st\n",
    "        current_layer_size = nn_structure[layer_size_index]\n",
    "        #previous layer start at 0th (input)\n",
    "        previous_layer_size = nn_structure[layer_size_index - 1]\n",
    "\n",
    "        W = np.random.rand(current_layer_size, previous_layer_size) - 0.5\n",
    "        b = np.random.rand(current_layer_size, 1) - 0.5\n",
    "\n",
    "        weights.append(W)\n",
    "        biases.append(b)\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "def forward_propogate(weights, biases ,input_layer, functions):\n",
    "    \"\"\"forward propogates the NN using inserted params & functions, given the input layer\"\"\"\n",
    "\n",
    "    s = input_layer.copy() #just for easier notation\n",
    "    nn_length = len(weights)\n",
    "\n",
    "    #will be eventually returned\n",
    "    forward_propogation_params_A = [s]\n",
    "    forward_propogation_params_Z = []\n",
    "    \n",
    "    #forward propogates\n",
    "    for i in range(nn_length):\n",
    "        #Note that A_0 = s\n",
    "        #Z_n = W_n @ A_(n-1) + b_n\n",
    "        Z = weights[i] @ forward_propogation_params_A[-1] + biases[i]\n",
    "        \n",
    "        #A_n = activation function(Z_n)\n",
    "        A = functions[i](Z)\n",
    "        print(A)\n",
    "\n",
    "        forward_propogation_params_Z.append(Z)\n",
    "        forward_propogation_params_A.append(A)\n",
    "\n",
    "    \n",
    "    \n",
    "    return forward_propogation_params_A, forward_propogation_params_Z\n",
    "\n",
    "\n",
    "def back_propogate(forward_propogation_params_A, forward_propogation_params_Z, weights, actions, targets, functions_deriv):\n",
    "    \"\"\"\n",
    "    weights: list of weight matrices\n",
    "    forward_propogation_params_A: list of all A matrices from forward_propagation\n",
    "    forward_propogation_params_Z: list of all Z matrices from forward_propagation\n",
    "    actions: array of action indices taken (batch_size,)\n",
    "    targets: array of target y-values (batch_size,)\n",
    "    functions_deriv: list of derivative functions for each layer\n",
    "    \"\"\"\n",
    "    #note this is written with a decent amount of help from AI\n",
    "    #I udnerstand most of this tho, but some of the matrix calculus is confusing\n",
    "    #AI is unmatched sometimes, it would take a human a long long long time to figure this out\n",
    "\n",
    "\n",
    "    m = actions.shape[0] #size of the batch\n",
    "\n",
    "    #gradient storate will later be returned\n",
    "    dW_list = []\n",
    "    db_list = []\n",
    "\n",
    "    #grab output layer\n",
    "    final_A = forward_propogation_params_A[-1]\n",
    "\n",
    "    #set everything else to 0, except for the decision which was made\n",
    "    dZ = np.zeros_like(final_A)\n",
    "    batch_indices = np.arange(m)\n",
    "    predictions = final_A[actions, batch_indices]\n",
    "    dZ[actions, batch_indices] = predictions - targets\n",
    "\n",
    "    #backpropogate through the layers until we get to the input layer\n",
    "    for i in range(len(weights) -1, 0, -1):\n",
    "        #a bunch of matrix calculus I am semi familiar with, but want to understand better\n",
    "        A_prev = forward_propogation_params_A[i] \n",
    "\n",
    "        dW = 1/m * dZ @ A_prev.T\n",
    "        db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "        dW_list.insert(0, dW)\n",
    "        db_list.insert(0, db)\n",
    "\n",
    "\n",
    "        if i > 0:\n",
    "            dZ = (weights[i].T @ dZ) * functions_deriv[i - 1](forward_propogation_params_Z[i - 1])\n",
    "\n",
    "    return dW_list, db_list\n",
    "\n",
    "def update_params(weights, biases, back_propogration_weights, back_propogration_biases, alpha):\n",
    "    \"\"\"updates all the params in the neural netwrok\"\"\"\n",
    "\n",
    "    nn_length = len(weights)\n",
    "\n",
    "    \n",
    "    #update everything\n",
    "    for i in range(nn_length):\n",
    "        #split for notation\n",
    "        W = weights[i]\n",
    "        b = biases[i]\n",
    "\n",
    "        dW = back_propogration_weights[i]\n",
    "        db = back_propogration_biases[i]\n",
    "\n",
    "\n",
    "        #update the biases\n",
    "        W -= alpha * dW\n",
    "        b -= alpha * db\n",
    "\n",
    "    return weights, biases #Since arrays are pointers, I can just return the orignial list of arrays\n",
    "\n",
    "def train_step(main_model_weights, main_model_biases, target_model_weights, target_model_biases, batch, functions, function_derivs, gamma, alpha):\n",
    "\n",
    "    #this function is also written with the help of AI\n",
    "\n",
    "    #unpack the batch\n",
    "    s, a, r, s_next, done = batch\n",
    "\n",
    "    #get Targets\n",
    "    forward_propogation_params_A_next, _ = forward_propogate(target_model_weights, target_model_biases, s_next, functions)\n",
    "    q_next_max = np.max(forward_propogation_params_A_next, axis=0)\n",
    "\n",
    "    #Calculate targets using bellman equation:\n",
    "    targets = r + (gamma * q_next_max * ( 1 - done)) #so for everything it's just gonna be r + gamme * q_next_max other then for the very last layer\n",
    "\n",
    "    #Forward pass through the main network\n",
    "    #This is required for backprop\n",
    "    forward_propogation_params_A, forward_propogation_params_Z = forward_propogate(main_model_weights, main_model_biases, s, functions)\n",
    "\n",
    "    #Backpropogate\n",
    "    back_propogration_weights, back_propogration_biases = back_propogate(forward_propogation_params_A, forward_propogation_params_Z, main_model_weights, a, targets, function_derivs)\n",
    "\n",
    "    \n",
    "    #Update weights\n",
    "    main_model_weights, main_model_biases = update_params(main_model_weights, main_model_biases, back_propogration_weights, back_propogration_biases, alpha)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
