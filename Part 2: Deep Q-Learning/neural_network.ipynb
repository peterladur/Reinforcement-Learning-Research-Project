{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c204aee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67237b4a",
   "metadata": {},
   "source": [
    "For deep q learning, there are a couple of differences in back prop we have to set up, but overall it's very similar to a typical NN setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc6564",
   "metadata": {},
   "source": [
    "Helper Functions Set Up:\n",
    "\n",
    "    - ReLU\n",
    "\n",
    "    - softmax\n",
    "\n",
    "    - derivative of ReLU\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c94c27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(Z):\n",
    "    \"\"\"Applies ReLU\"\"\"\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"Applies softmax\"\"\"\n",
    "    return np.exp(Z)/np.sum(np.exp(Z))\n",
    "\n",
    "def deriv_ReLU(Z):\n",
    "    \"\"\"Diffirentiates ReLU\"\"\"\n",
    "    return Z > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0b2ace",
   "metadata": {},
   "source": [
    "NN functionality:\n",
    "\n",
    "    - init params\n",
    "\n",
    "    - forward prop\n",
    "\n",
    "    - back prop\n",
    "\n",
    "    - updating params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5176c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(nn_structure:list):\n",
    "    \"\"\"This function sets up initial parameters W1, b1, W2, b2, ... following the given structure\"\"\"\n",
    "\n",
    "    #weights and biases will contain np matrices of the weights and biases for each layer such that\n",
    "    #weight[n] = n'th layer weights, etc\n",
    "    weights = []\n",
    "    biases = []\n",
    "    \n",
    "    for layer_size_index in range(1, len(nn_structure)):\n",
    "        \n",
    "        #current layer starting at 1st\n",
    "        current_layer_size = nn_structure[layer_size_index]\n",
    "        #previous layer start at 0th (input)\n",
    "        previous_layer_size = nn_structure[layer_size_index - 1]\n",
    "\n",
    "        W = np.random.rand(current_layer_size, previous_layer_size) - 0.5\n",
    "        b = np.random.rand(current_layer_size, 1) - 0.5\n",
    "\n",
    "        weights.append(W)\n",
    "        biases.append(b)\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "def forward_propogate(weights, biases ,input_layer, functions):\n",
    "    \"\"\"forward propogates the NN using inserted params & functions, given the input layer\"\"\"\n",
    "\n",
    "    s = input_layer.copy() #just for easier notation\n",
    "    nn_length = len(weights)\n",
    "\n",
    "    #will be eventually returned (with X removed)\n",
    "    forward_propogation_params_A = [s]\n",
    "    forward_propogation_params_Z = []\n",
    "    \n",
    "    #forward propogates\n",
    "    for i in range(nn_length):\n",
    "        #Note that A_0 = s\n",
    "        #Z_n = W_n @ A_(n-1) + b_n\n",
    "        Z = weights[i] @ forward_propogation_params_A[-1] + biases[i]\n",
    "        \n",
    "        #A_n = activation function(Z_n)\n",
    "        A = functions[i](Z)\n",
    "        print(A)\n",
    "\n",
    "        forward_propogation_params_Z.append(Z)\n",
    "        forward_propogation_params_A.append(A)\n",
    "\n",
    "    forward_propogation_params_A.pop(0) #removes s\n",
    "    \n",
    "    \n",
    "    return forward_propogation_params_A, forward_propogation_params_Z\n",
    "\n",
    "\n",
    "def back_propogate(forward_propogation_params_A, forward_propogation_params_Z, weights, batch_params, functions, gamma):\n",
    "    \"\"\"propogates backwards through the network using gradient descent\"\"\"\n",
    "\n",
    "    #unpack batch\n",
    "    s = batch_params[0]\n",
    "    a = batch_params[1]\n",
    "    r = batch_params[2]\n",
    "    s_next = batch_params[3]\n",
    "\n",
    "\n",
    "    #get the Q(s, a, theta)\n",
    "    final_layer = forward_propogation_params_A[-1]\n",
    "    Q_s_value = final_layer[a]\n",
    "\n",
    "    #get the max(Q(s', a', theta-))\n",
    "    Q_s_next_value = 1#np.max(forward_propogate(, s_next, functions)[0][-1]) #gets the largest entry from A_last\n",
    "    \n",
    "    #calculates error:\n",
    "        #loss = (r + gamma * max(Q(s', a', theta-) - Q(s, a, theta))**2\n",
    "\n",
    "    loss = 0.5 * (r + gamma * Q_s_next_value - Q_s_value)**2\n",
    "\n",
    "\n",
    "    #back propogation\n",
    "\n",
    "\n",
    "def update_params(weights, biases, back_propogration_weights, back_propogration_biases, alpha):\n",
    "    \"\"\"updates all the params in the neural netwrok\"\"\"\n",
    "\n",
    "    nn_length = len(weights)\n",
    "\n",
    "    \n",
    "    #update everything\n",
    "    for i in range(nn_length):\n",
    "        #split for notation\n",
    "        W = weights[i]\n",
    "        b = biases[i]\n",
    "\n",
    "        dW = back_propogration_weights[i]\n",
    "        db = back_propogration_biases[i]\n",
    "\n",
    "\n",
    "        #update the biases\n",
    "        W -= alpha * dW\n",
    "        b -= alpha * db\n",
    "\n",
    "    return weights, biases #Since arrays are pointers, I can just return the orignial list of arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11506c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "[[0.         0.36224339 0.         0.34682175 0.        ]\n",
      " [0.         0.46186242 0.         0.44644079 0.        ]\n",
      " [0.         0.41658598 0.         0.40116435 0.        ]\n",
      " [0.         0.04258405 0.         0.02716242 0.        ]\n",
      " [0.         0.21030402 0.         0.19488238 0.        ]]\n",
      "[[0.14045283 0.1418157  0.14045283 0.1422286  0.14045283]\n",
      " [0.05869086 0.05908179 0.05869086 0.05944284 0.05869086]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([[0.        , 0.36224339, 0.        , 0.34682175, 0.        ],\n",
       "         [0.        , 0.46186242, 0.        , 0.44644079, 0.        ],\n",
       "         [0.        , 0.41658598, 0.        , 0.40116435, 0.        ],\n",
       "         [0.        , 0.04258405, 0.        , 0.02716242, 0.        ],\n",
       "         [0.        , 0.21030402, 0.        , 0.19488238, 0.        ]]),\n",
       "  array([[0.14045283, 0.1418157 , 0.14045283, 0.1422286 , 0.14045283],\n",
       "         [0.05869086, 0.05908179, 0.05869086, 0.05944284, 0.05869086]])],\n",
       " [array([[-0.5209631 ,  0.36224339, -0.53561756,  0.34682175, -1.61161614],\n",
       "         [-0.42134406,  0.46186242, -0.43599853,  0.44644079, -1.5119971 ],\n",
       "         [-0.4666205 ,  0.41658598, -0.48127497,  0.40116435, -1.55727354],\n",
       "         [-0.84062243,  0.04258405, -0.8552769 ,  0.02716242, -1.93127547],\n",
       "         [-0.67290247,  0.21030402, -0.68755693,  0.19488238, -1.76355551]]),\n",
       "  array([[ 0.3962521 ,  0.40590866,  0.3962521 ,  0.408816  ,  0.3962521 ],\n",
       "         [-0.47633571, -0.46969686, -0.47633571, -0.46360441, -0.47633571]])])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_structue = [3, 5, 2]\n",
    "\n",
    "functions = [ReLU, softmax]\n",
    "\n",
    "weights, biases = init_params(nn_structue)\n",
    "\n",
    "X = np.array([[1, 2, 3]]).T\n",
    "print(X)\n",
    "forward_propogate(weights, biases,X, functions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124c703c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
